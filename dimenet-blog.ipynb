{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01014d5c",
   "metadata": {},
   "source": [
    "## Annotated DimeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4565445",
   "metadata": {},
   "source": [
    "A Step-by-step process to build a graph neural network using Pytorch for Molecule Property Prediction. This blog will go through the process of building DimeNet, we'll elaborate how each components is constructed and how they piece back together towards the end.\n",
    "\n",
    "Feel free to go through this inside a google colab to play with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2df8e",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06df215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/arc/project/st-jiaruid-1/miniconda3/envs/gnn/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from math import pi as PI\n",
    "from math import sqrt\n",
    "from typing import Callable, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, Linear\n",
    "from torch_scatter import scatter\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "from torch_geometric.data import Dataset, download_url\n",
    "from torch_geometric.data.makedirs import makedirs\n",
    "from torch_geometric.nn import radius_graph\n",
    "from torch_geometric.nn.inits import glorot_orthogonal\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.typing import OptTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652fb56c",
   "metadata": {},
   "source": [
    "### Directional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04558957",
   "metadata": {},
   "source": [
    "The key idea in this paper is to use directional information and perform message passing on it. The paper then uses spherical Bessel functions and radial basis functions to transform the directional information into a real, orthogonal basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434dae3b",
   "metadata": {},
   "source": [
    "### Envelope\n",
    "This envelope ensures the $d$ (interatomic distance) is differentiable at ($d = c$)\n",
    "$$\n",
    "u(d) = 1 - \\frac{(p + 1)(p + 2)}{2}d^p + p(p + 2)d^{p+1} - \\frac{p(p + 1)}{2}d^{p + 2}\n",
    "$$\n",
    "\n",
    "The paper uses a default value of p = 6 or `exponent = 5.` in the code.\n",
    "\n",
    "We implement this as a layer (although no parameters are being learned in this layer). The `exponent` variable corresponds to the $p$ in the above equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44faae0",
   "metadata": {},
   "source": [
    "TODO: See why we are using $p - 1$ exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f5e8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Envelope(torch.nn.Module):\n",
    "    def __init__(self, exponent: int):\n",
    "        super().__init__()\n",
    "        self.p = exponent + 1\n",
    "        self.a = -(self.p + 1) * (self.p + 2) / 2\n",
    "        self.b = self.p * (self.p + 2)\n",
    "        self.c = -self.p * (self.p + 1) / 2\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        p, a, b, c = self.p, self.a, self.b, self.c\n",
    "        x_pow_p0 = x.pow(p - 1)\n",
    "        x_pow_p1 = x_pow_p0 * x\n",
    "        x_pow_p2 = x_pow_p1 * x\n",
    "        return (\n",
    "            1. / x + a * x_pow_p0 + b * x_pow_p1 +\n",
    "            c * x_pow_p2) * (x < 1.0).to(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db48ec4a",
   "metadata": {},
   "source": [
    "### Bessel Basis Layer\n",
    "$$\n",
    "\\mathcal{e}_{RBF}(d) = u(d)\\tilde{\\mathcal{e}}_{RBF}(d) \\\\\n",
    "\\tilde{\\mathcal{e}}_{RBF}(d) = \\sqrt{\\frac{2}{c}}\\frac{\\sin{(\\frac{n\\pi}{c}}d)}{d}\n",
    "$$\n",
    "The value of the $N_{RBF}$, denotes the number of real and orthogonal bases to be used, $n \\in [1, ... , N_{RBF}]$\n",
    "We implement the Bessel Basis as a layer where the frequency information is to be learned. \n",
    "\n",
    "The following mapping from the math variables and to code variables will be used,\n",
    "- $N_{RBF} \\to $ `num_radial` \n",
    "- $c \\to $ `cutoff`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6b576",
   "metadata": {},
   "source": [
    "### [Remove after solving] Questions for Bessel Basis Layer\n",
    "- why not reset_parameters() with, torch.nn.Parameter(torch.arange(1, self.num_radial + 1).float())\n",
    "- why are we doing dist / self.cutoff in the forward loop and passing that d to the envelope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BesselBasisLayer(torch.nn.Module):\n",
    "    '''Bessel Basis Layer'''\n",
    "    def __init__(self, num_radial: int, cutoff: float = 5.0,\n",
    "                 envelope_exponent: int = 5):\n",
    "        super().__init__()\n",
    "        # the c in the bessel basis equation\n",
    "        self.cutoff = cutoff\n",
    "        # envelope as in the above code block to make it twice differentiable\n",
    "        self.envelope = Envelope(envelope_exponent)\n",
    "        \n",
    "        # the different frequencies to be considered to generate orthogonal basis\n",
    "        self.freq = torch.nn.Parameter(torch.Tensor(num_radial))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        with torch.no_grad():\n",
    "            torch.arange(1, self.freq.numel() + 1, out=self.freq).mul_(PI)\n",
    "        self.freq.requires_grad_()\n",
    "\n",
    "    def forward(self, dist: Tensor) -> Tensor:\n",
    "        dist = (dist.unsqueeze(-1) / self.cutoff)\n",
    "        return self.envelope(dist) * (self.freq * dist).sin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c5b16",
   "metadata": {},
   "source": [
    "#### Spherical Basis Layer\n",
    "This basis layer is for joint 2D basis for $d_{kj}$ and $\\alpha_{(kj,ji)}$, a function dependent on the interatomic distance and an angle.\n",
    "\n",
    "$$\n",
    "\\mathcal{a}_{RBF}(d) = u(d) \\tilde{\\mathcal{a}}_{RBF}(d) \\\\\n",
    "$$\n",
    "$$\n",
    "\\tilde{\\mathcal{a}}_{SBF, ln}(d, \\alpha) = \\sqrt{\\frac{2}{c^3 j^2_{l + 1}(z_{ln})}} j_l(\\frac{z_{ln}}{c}d)Y_l^0(\\alpha)\n",
    "$$\n",
    "where $l \\in [0 .. N_{SHBF} - 1]$ and $n \\in [1 ... N_{SRBF}]$\n",
    "\n",
    "The following mapping from the math variables and to code variables will be used,\n",
    "- $l \\to $ `num_spherical` \n",
    "- $n \\to $ `num_radial` \n",
    "- $c \\to $ `cutoff`\n",
    "\n",
    "The next two are actually pre-computed using `torch_geometric.nn.models.dimenet_utils`\n",
    "- $z_{ln}$ : $n$th root of the $l$-order bessel function using `bessel_basis(num_spherical, num_radial)`\n",
    "- $Y_l^0(\\alpha)$ : Spherical Harmonics using `real_sph_harm(num_spherical)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8d1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalBasisLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_spherical: int, \n",
    "        num_radial: int,\n",
    "        cutoff: float = 5.0, \n",
    "        envelope_exponent: int = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        import sympy as sym\n",
    "\n",
    "        from torch_geometric.nn.models.dimenet_utils import (\n",
    "            bessel_basis,\n",
    "            real_sph_harm,\n",
    "        )\n",
    "\n",
    "        assert num_radial <= 64\n",
    "        self.num_spherical = num_spherical\n",
    "        self.num_radial = num_radial\n",
    "        self.cutoff = cutoff\n",
    "        self.envelope = Envelope(envelope_exponent)\n",
    "\n",
    "        bessel_forms = bessel_basis(num_spherical, num_radial)\n",
    "        sph_harm_forms = real_sph_harm(num_spherical)\n",
    "        self.sph_funcs = []\n",
    "        self.bessel_funcs = []\n",
    "\n",
    "        x, theta = sym.symbols('x theta')\n",
    "        modules = {'sin': torch.sin, 'cos': torch.cos}\n",
    "        for i in range(num_spherical):\n",
    "            if i == 0:\n",
    "                sph1 = sym.lambdify([theta], sph_harm_forms[i][0], modules)(0)\n",
    "                self.sph_funcs.append(lambda x: torch.zeros_like(x) + sph1)\n",
    "            else:\n",
    "                sph = sym.lambdify([theta], sph_harm_forms[i][0], modules)\n",
    "                self.sph_funcs.append(sph)\n",
    "            for j in range(num_radial):\n",
    "                bessel = sym.lambdify([x], bessel_forms[i][j], modules)\n",
    "                self.bessel_funcs.append(bessel)\n",
    "\n",
    "    def forward(self, dist: Tensor, angle: Tensor, idx_kj: Tensor) -> Tensor:\n",
    "        '''Performs Forward Pass'''\n",
    "        # computes d / c\n",
    "        dist = dist / self.cutoff\n",
    "        rbf = torch.stack([f(dist) for f in self.bessel_funcs], dim=1)\n",
    "        rbf = self.envelope(dist).unsqueeze(-1) * rbf\n",
    "\n",
    "        cbf = torch.stack([f(angle) for f in self.sph_funcs], dim=1)\n",
    "\n",
    "        n, k = self.num_spherical, self.num_radial\n",
    "        out = (rbf[idx_kj].view(-1, n, k) * cbf.view(-1, n, 1)).view(-1, n * k)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87846ead",
   "metadata": {},
   "source": [
    "### Embedding Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(torch.nn.Module):\n",
    "    def __init__(self, num_radial: int, hidden_channels: int, act: Callable):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "\n",
    "        self.emb = Embedding(95, hidden_channels)\n",
    "        self.lin_rbf = Linear(num_radial, hidden_channels)\n",
    "        self.lin = Linear(3 * hidden_channels, hidden_channels)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.emb.weight.data.uniform_(-sqrt(3), sqrt(3))\n",
    "        self.lin_rbf.reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor, j: Tensor) -> Tensor:\n",
    "        x = self.emb(x)\n",
    "        rbf = self.act(self.lin_rbf(rbf))\n",
    "        return self.act(self.lin(torch.cat([x[i], x[j], rbf], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c592b73",
   "metadata": {},
   "source": [
    "### Interaction Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215bc779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels: int, num_bilinear: int,\n",
    "                 num_spherical: int, num_radial: int, num_before_skip: int,\n",
    "                 num_after_skip: int, act: Callable):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "\n",
    "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
    "        self.lin_sbf = Linear(num_spherical * num_radial, num_bilinear,\n",
    "                              bias=False)\n",
    "\n",
    "        # Dense transformations of input messages.\n",
    "        self.lin_kj = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin_ji = Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.W = torch.nn.Parameter(\n",
    "            torch.Tensor(hidden_channels, num_bilinear, hidden_channels))\n",
    "\n",
    "        self.layers_before_skip = torch.nn.ModuleList([\n",
    "            ResidualLayer(hidden_channels, act) for _ in range(num_before_skip)\n",
    "        ])\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.layers_after_skip = torch.nn.ModuleList([\n",
    "            ResidualLayer(hidden_channels, act) for _ in range(num_after_skip)\n",
    "        ])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_sbf.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_kj.weight, scale=2.0)\n",
    "        self.lin_kj.bias.data.fill_(0)\n",
    "        glorot_orthogonal(self.lin_ji.weight, scale=2.0)\n",
    "        self.lin_ji.bias.data.fill_(0)\n",
    "        self.W.data.normal_(mean=0, std=2 / self.W.size(0))\n",
    "        for res_layer in self.layers_before_skip:\n",
    "            res_layer.reset_parameters()\n",
    "        glorot_orthogonal(self.lin.weight, scale=2.0)\n",
    "        self.lin.bias.data.fill_(0)\n",
    "        for res_layer in self.layers_after_skip:\n",
    "            res_layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Tensor, rbf: Tensor, sbf: Tensor, idx_kj: Tensor,\n",
    "                idx_ji: Tensor) -> Tensor:\n",
    "        rbf = self.lin_rbf(rbf)\n",
    "        sbf = self.lin_sbf(sbf)\n",
    "\n",
    "        x_ji = self.act(self.lin_ji(x))\n",
    "        x_kj = self.act(self.lin_kj(x))\n",
    "        x_kj = x_kj * rbf\n",
    "        x_kj = torch.einsum('wj,wl,ijl->wi', sbf, x_kj[idx_kj], self.W)\n",
    "        x_kj = scatter(x_kj, idx_ji, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        h = x_ji + x_kj\n",
    "        for layer in self.layers_before_skip:\n",
    "            h = layer(h)\n",
    "        h = self.act(self.lin(h)) + x\n",
    "        for layer in self.layers_after_skip:\n",
    "            h = layer(h)\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d106294",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "260b97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputBlock(torch.nn.Module):\n",
    "    def __init__(self, num_radial: int, hidden_channels: int,\n",
    "                 out_channels: int, num_layers: int, act: Callable):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "\n",
    "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.lins.append(Linear(hidden_channels, hidden_channels))\n",
    "        self.lin = Linear(hidden_channels, out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
    "        for lin in self.lins:\n",
    "            glorot_orthogonal(lin.weight, scale=2.0)\n",
    "            lin.bias.data.fill_(0)\n",
    "        self.lin.weight.data.fill_(0)\n",
    "\n",
    "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor,\n",
    "                num_nodes: Optional[int] = None) -> Tensor:\n",
    "        x = self.lin_rbf(rbf) * x\n",
    "        x = scatter(x, i, dim=0, dim_size=num_nodes)\n",
    "        for lin in self.lins:\n",
    "            x = self.act(lin(x))\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68efeb",
   "metadata": {},
   "source": [
    "### Residual Layer\n",
    "This is similar to what is used in ResNets.\n",
    "\n",
    "TODO: Put diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels: int, act: Callable):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot_orthogonal(self.lin1.weight, scale=2.0)\n",
    "        self.lin1.bias.data.fill_(0)\n",
    "        glorot_orthogonal(self.lin2.weight, scale=2.0)\n",
    "        self.lin2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x + self.act(self.lin2(self.act(self.lin1(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28223449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9ee350b",
   "metadata": {},
   "source": [
    "### DimeNet: Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2de2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimeNet(torch.nn.Module):\n",
    "    r\"\"\"The directional message passing neural network (DimeNet) from the\n",
    "    `\"Directional Message Passing for Molecular Graphs\"\n",
    "    <https://arxiv.org/abs/2003.03123>`_ paper.\n",
    "    DimeNet transforms messages based on the angle between them in a\n",
    "    rotation-equivariant fashion.\n",
    "    .. note::\n",
    "        For an example of using a pretrained DimeNet variant, see\n",
    "        `examples/qm9_pretrained_dimenet.py\n",
    "        <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/\n",
    "        qm9_pretrained_dimenet.py>`_.\n",
    "    Args:\n",
    "        hidden_channels (int): Hidden embedding size.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_blocks (int): Number of building blocks.\n",
    "        num_bilinear (int): Size of the bilinear layer tensor.\n",
    "        num_spherical (int): Number of spherical harmonics.\n",
    "        num_radial (int): Number of radial basis functions.\n",
    "        cutoff (float, optional): Cutoff distance for interatomic\n",
    "            interactions. (default: :obj:`5.0`)\n",
    "        max_num_neighbors (int, optional): The maximum number of neighbors to\n",
    "            collect for each node within the :attr:`cutoff` distance.\n",
    "            (default: :obj:`32`)\n",
    "        envelope_exponent (int, optional): Shape of the smooth cutoff.\n",
    "            (default: :obj:`5`)\n",
    "        num_before_skip (int, optional): Number of residual layers in the\n",
    "            interaction blocks before the skip connection. (default: :obj:`1`)\n",
    "        num_after_skip (int, optional): Number of residual layers in the\n",
    "            interaction blocks after the skip connection. (default: :obj:`2`)\n",
    "        num_output_layers (int, optional): Number of linear layers for the\n",
    "            output blocks. (default: :obj:`3`)\n",
    "        act (str or Callable, optional): The activation function.\n",
    "            (default: :obj:`\"swish\"`)\n",
    "    \"\"\"\n",
    "\n",
    "    url = ('https://github.com/klicperajo/dimenet/raw/master/pretrained/'\n",
    "           'dimenet')\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        num_blocks: int,\n",
    "        num_bilinear: int,\n",
    "        num_spherical: int,\n",
    "        num_radial,\n",
    "        cutoff: float = 5.0,\n",
    "        max_num_neighbors: int = 32,\n",
    "        envelope_exponent: int = 5,\n",
    "        num_before_skip: int = 1,\n",
    "        num_after_skip: int = 2,\n",
    "        num_output_layers: int = 3,\n",
    "        act: Union[str, Callable] = 'swish',\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_spherical < 2:\n",
    "            raise ValueError(\"num_spherical should be greater than 1\")\n",
    "\n",
    "        act = activation_resolver(act)\n",
    "\n",
    "        self.cutoff = cutoff\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        self.rbf = BesselBasisLayer(num_radial, cutoff, envelope_exponent)\n",
    "        self.sbf = SphericalBasisLayer(num_spherical, num_radial, cutoff,\n",
    "                                       envelope_exponent)\n",
    "\n",
    "        self.emb = EmbeddingBlock(num_radial, hidden_channels, act)\n",
    "\n",
    "        self.output_blocks = torch.nn.ModuleList([\n",
    "            OutputBlock(num_radial, hidden_channels, out_channels,\n",
    "                        num_output_layers, act) for _ in range(num_blocks + 1)\n",
    "        ])\n",
    "\n",
    "        self.interaction_blocks = torch.nn.ModuleList([\n",
    "            InteractionBlock(hidden_channels, num_bilinear, num_spherical,\n",
    "                             num_radial, num_before_skip, num_after_skip, act)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.rbf.reset_parameters()\n",
    "        self.emb.reset_parameters()\n",
    "        for out in self.output_blocks:\n",
    "            out.reset_parameters()\n",
    "        for interaction in self.interaction_blocks:\n",
    "            interaction.reset_parameters()\n",
    "\n",
    "    @classmethod\n",
    "    def from_qm9_pretrained(\n",
    "        cls,\n",
    "        root: str,\n",
    "        dataset: Dataset,\n",
    "        target: int,\n",
    "    ) -> Tuple['DimeNet', Dataset, Dataset, Dataset]:\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "        import tensorflow as tf\n",
    "\n",
    "        assert target >= 0 and target <= 12 and not target == 4\n",
    "\n",
    "        root = osp.expanduser(osp.normpath(root))\n",
    "        path = osp.join(root, 'pretrained_dimenet', qm9_target_dict[target])\n",
    "\n",
    "        makedirs(path)\n",
    "        url = f'{cls.url}/{qm9_target_dict[target]}'\n",
    "\n",
    "        if not osp.exists(osp.join(path, 'checkpoint')):\n",
    "            download_url(f'{url}/checkpoint', path)\n",
    "            download_url(f'{url}/ckpt.data-00000-of-00002', path)\n",
    "            download_url(f'{url}/ckpt.data-00001-of-00002', path)\n",
    "            download_url(f'{url}/ckpt.index', path)\n",
    "\n",
    "        path = osp.join(path, 'ckpt')\n",
    "        reader = tf.train.load_checkpoint(path)\n",
    "\n",
    "        model = cls(\n",
    "            hidden_channels=128,\n",
    "            out_channels=1,\n",
    "            num_blocks=6,\n",
    "            num_bilinear=8,\n",
    "            num_spherical=7,\n",
    "            num_radial=6,\n",
    "            cutoff=5.0,\n",
    "            envelope_exponent=5,\n",
    "            num_before_skip=1,\n",
    "            num_after_skip=2,\n",
    "            num_output_layers=3,\n",
    "        )\n",
    "\n",
    "        def copy_(src, name, transpose=False):\n",
    "            init = reader.get_tensor(f'{name}/.ATTRIBUTES/VARIABLE_VALUE')\n",
    "            init = torch.from_numpy(init)\n",
    "            if name[-6:] == 'kernel':\n",
    "                init = init.t()\n",
    "            src.data.copy_(init)\n",
    "\n",
    "        copy_(model.rbf.freq, 'rbf_layer/frequencies')\n",
    "        copy_(model.emb.emb.weight, 'emb_block/embeddings')\n",
    "        copy_(model.emb.lin_rbf.weight, 'emb_block/dense_rbf/kernel')\n",
    "        copy_(model.emb.lin_rbf.bias, 'emb_block/dense_rbf/bias')\n",
    "        copy_(model.emb.lin.weight, 'emb_block/dense/kernel')\n",
    "        copy_(model.emb.lin.bias, 'emb_block/dense/bias')\n",
    "\n",
    "        for i, block in enumerate(model.output_blocks):\n",
    "            copy_(block.lin_rbf.weight, f'output_blocks/{i}/dense_rbf/kernel')\n",
    "            for j, lin in enumerate(block.lins):\n",
    "                copy_(lin.weight, f'output_blocks/{i}/dense_layers/{j}/kernel')\n",
    "                copy_(lin.bias, f'output_blocks/{i}/dense_layers/{j}/bias')\n",
    "            copy_(block.lin.weight, f'output_blocks/{i}/dense_final/kernel')\n",
    "\n",
    "        for i, block in enumerate(model.interaction_blocks):\n",
    "            copy_(block.lin_rbf.weight, f'int_blocks/{i}/dense_rbf/kernel')\n",
    "            copy_(block.lin_sbf.weight, f'int_blocks/{i}/dense_sbf/kernel')\n",
    "            copy_(block.lin_kj.weight, f'int_blocks/{i}/dense_kj/kernel')\n",
    "            copy_(block.lin_kj.bias, f'int_blocks/{i}/dense_kj/bias')\n",
    "            copy_(block.lin_ji.weight, f'int_blocks/{i}/dense_ji/kernel')\n",
    "            copy_(block.lin_ji.bias, f'int_blocks/{i}/dense_ji/bias')\n",
    "            copy_(block.W, f'int_blocks/{i}/bilinear')\n",
    "            for j, layer in enumerate(block.layers_before_skip):\n",
    "                copy_(layer.lin1.weight,\n",
    "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_1/kernel')\n",
    "                copy_(layer.lin1.bias,\n",
    "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_1/bias')\n",
    "                copy_(layer.lin2.weight,\n",
    "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_2/kernel')\n",
    "                copy_(layer.lin2.bias,\n",
    "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_2/bias')\n",
    "            copy_(block.lin.weight, f'int_blocks/{i}/final_before_skip/kernel')\n",
    "            copy_(block.lin.bias, f'int_blocks/{i}/final_before_skip/bias')\n",
    "            for j, layer in enumerate(block.layers_after_skip):\n",
    "                copy_(layer.lin1.weight,\n",
    "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_1/kernel')\n",
    "                copy_(layer.lin1.bias,\n",
    "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_1/bias')\n",
    "                copy_(layer.lin2.weight,\n",
    "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_2/kernel')\n",
    "                copy_(layer.lin2.bias,\n",
    "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_2/bias')\n",
    "\n",
    "        # Use the same random seed as the official DimeNet` implementation.\n",
    "        random_state = np.random.RandomState(seed=42)\n",
    "        perm = torch.from_numpy(random_state.permutation(np.arange(130831)))\n",
    "        train_idx = perm[:110000]\n",
    "        val_idx = perm[110000:120000]\n",
    "        test_idx = perm[120000:]\n",
    "\n",
    "        return model, (dataset[train_idx], dataset[val_idx], dataset[test_idx])\n",
    "\n",
    "    def triplets(\n",
    "        self,\n",
    "        edge_index: Tensor,\n",
    "        num_nodes: int,\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
    "        row, col = edge_index  # j->i\n",
    "\n",
    "        value = torch.arange(row.size(0), device=row.device)\n",
    "        adj_t = SparseTensor(row=col, col=row, value=value,\n",
    "                             sparse_sizes=(num_nodes, num_nodes))\n",
    "        adj_t_row = adj_t[row]\n",
    "        num_triplets = adj_t_row.set_value(None).sum(dim=1).to(torch.long)\n",
    "\n",
    "        # Node indices (k->j->i) for triplets.\n",
    "        idx_i = col.repeat_interleave(num_triplets)\n",
    "        idx_j = row.repeat_interleave(num_triplets)\n",
    "        idx_k = adj_t_row.storage.col()\n",
    "        mask = idx_i != idx_k  # Remove i == k triplets.\n",
    "        idx_i, idx_j, idx_k = idx_i[mask], idx_j[mask], idx_k[mask]\n",
    "\n",
    "        # Edge indices (k-j, j->i) for triplets.\n",
    "        idx_kj = adj_t_row.storage.value()[mask]\n",
    "        idx_ji = adj_t_row.storage.row()[mask]\n",
    "\n",
    "        return col, row, idx_i, idx_j, idx_k, idx_kj, idx_ji\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        z: Tensor,\n",
    "        pos: Tensor,\n",
    "        batch: OptTensor = None,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        edge_index = radius_graph(pos, r=self.cutoff, batch=batch,\n",
    "                                  max_num_neighbors=self.max_num_neighbors)\n",
    "\n",
    "        i, j, idx_i, idx_j, idx_k, idx_kj, idx_ji = self.triplets(\n",
    "            edge_index, num_nodes=z.size(0))\n",
    "\n",
    "        # Calculate distances.\n",
    "        dist = (pos[i] - pos[j]).pow(2).sum(dim=-1).sqrt()\n",
    "\n",
    "        # Calculate angles.\n",
    "        pos_i = pos[idx_i]\n",
    "        pos_ji, pos_ki = pos[idx_j] - pos_i, pos[idx_k] - pos_i\n",
    "        a = (pos_ji * pos_ki).sum(dim=-1)\n",
    "        b = torch.cross(pos_ji, pos_ki).norm(dim=-1)\n",
    "        angle = torch.atan2(b, a)\n",
    "\n",
    "        rbf = self.rbf(dist)\n",
    "        sbf = self.sbf(dist, angle, idx_kj)\n",
    "\n",
    "        # Embedding block.\n",
    "        x = self.emb(z, rbf, i, j)\n",
    "        P = self.output_blocks[0](x, rbf, i, num_nodes=pos.size(0))\n",
    "\n",
    "        # Interaction blocks.\n",
    "        for interaction_block, output_block in zip(self.interaction_blocks,\n",
    "                                                   self.output_blocks[1:]):\n",
    "            x = interaction_block(x, rbf, sbf, idx_kj, idx_ji)\n",
    "            P = P + output_block(x, rbf, i)\n",
    "\n",
    "        return P.sum(dim=0) if batch is None else scatter(P, batch, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375ca3f",
   "metadata": {},
   "source": [
    "### DimeNet++ Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionPPBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels: int, int_emb_size: int,\n",
    "                 basis_emb_size: int, num_spherical: int, num_radial: int,\n",
    "                 num_before_skip: int, num_after_skip: int, act: Callable):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "\n",
    "        # Transformation of Bessel and spherical basis representations:\n",
    "        self.lin_rbf1 = Linear(num_radial, basis_emb_size, bias=False)\n",
    "        self.lin_rbf2 = Linear(basis_emb_size, hidden_channels, bias=False)\n",
    "\n",
    "        self.lin_sbf1 = Linear(num_spherical * num_radial, basis_emb_size,\n",
    "                               bias=False)\n",
    "        self.lin_sbf2 = Linear(basis_emb_size, int_emb_size, bias=False)\n",
    "\n",
    "        # Hidden transformation of input message:\n",
    "        self.lin_kj = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin_ji = Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Embedding projections for interaction triplets:\n",
    "        self.lin_down = Linear(hidden_channels, int_emb_size, bias=False)\n",
    "        self.lin_up = Linear(int_emb_size, hidden_channels, bias=False)\n",
    "\n",
    "        # Residual layers before and after skip connection:\n",
    "        self.layers_before_skip = torch.nn.ModuleList([\n",
    "            ResidualLayer(hidden_channels, act) for _ in range(num_before_skip)\n",
    "        ])\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.layers_after_skip = torch.nn.ModuleList([\n",
    "            ResidualLayer(hidden_channels, act) for _ in range(num_before_skip)\n",
    "        ])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot_orthogonal(self.lin_rbf1.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_rbf2.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_sbf1.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_sbf2.weight, scale=2.0)\n",
    "\n",
    "        glorot_orthogonal(self.lin_kj.weight, scale=2.0)\n",
    "        self.lin_kj.bias.data.fill_(0)\n",
    "        glorot_orthogonal(self.lin_ji.weight, scale=2.0)\n",
    "        self.lin_ji.bias.data.fill_(0)\n",
    "\n",
    "        glorot_orthogonal(self.lin_down.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_up.weight, scale=2.0)\n",
    "\n",
    "        for res_layer in self.layers_before_skip:\n",
    "            res_layer.reset_parameters()\n",
    "        glorot_orthogonal(self.lin.weight, scale=2.0)\n",
    "        self.lin.bias.data.fill_(0)\n",
    "        for res_layer in self.layers_before_skip:\n",
    "            res_layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Tensor, rbf: Tensor, sbf: Tensor, idx_kj: Tensor,\n",
    "                idx_ji: Tensor) -> Tensor:\n",
    "        # Initial transformation:\n",
    "        x_ji = self.act(self.lin_ji(x))\n",
    "        x_kj = self.act(self.lin_kj(x))\n",
    "\n",
    "        # Transformation via Bessel basis:\n",
    "        rbf = self.lin_rbf1(rbf)\n",
    "        rbf = self.lin_rbf2(rbf)\n",
    "        x_kj = x_kj * rbf\n",
    "\n",
    "        # Down project embedding and generating triple-interactions:\n",
    "        x_kj = self.act(self.lin_down(x_kj))\n",
    "\n",
    "        # Transform via 2D spherical basis:\n",
    "        sbf = self.lin_sbf1(sbf)\n",
    "        sbf = self.lin_sbf2(sbf)\n",
    "        x_kj = x_kj[idx_kj] * sbf\n",
    "\n",
    "        # Aggregate interactions and up-project embeddings:\n",
    "        x_kj = scatter(x_kj, idx_ji, dim=0, dim_size=x.size(0))\n",
    "        x_kj = self.act(self.lin_up(x_kj))\n",
    "\n",
    "        h = x_ji + x_kj\n",
    "        for layer in self.layers_before_skip:\n",
    "            h = layer(h)\n",
    "        h = self.act(self.lin(h)) + x\n",
    "        for layer in self.layers_after_skip:\n",
    "            h = layer(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "class OutputPPBlock(torch.nn.Module):\n",
    "    def __init__(self, num_radial: int, hidden_channels: int,\n",
    "                 out_emb_channels: int, out_channels: int, num_layers: int,\n",
    "                 act: Callable):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "\n",
    "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
    "\n",
    "        # The up-projection layer:\n",
    "        self.lin_up = Linear(hidden_channels, out_emb_channels, bias=False)\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.lins.append(Linear(out_emb_channels, out_emb_channels))\n",
    "        self.lin = Linear(out_emb_channels, out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_up.weight, scale=2.0)\n",
    "        for lin in self.lins:\n",
    "            glorot_orthogonal(lin.weight, scale=2.0)\n",
    "            lin.bias.data.fill_(0)\n",
    "        self.lin.weight.data.fill_(0)\n",
    "\n",
    "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor,\n",
    "                num_nodes: Optional[int] = None) -> Tensor:\n",
    "        x = self.lin_rbf(rbf) * x\n",
    "        x = scatter(x, i, dim=0, dim_size=num_nodes)\n",
    "        x = self.lin_up(x)\n",
    "        for lin in self.lins:\n",
    "            x = self.act(lin(x))\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884ea86",
   "metadata": {},
   "source": [
    "## DimeNet++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a83ebb62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DimeNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDimeNetPlusPlus\u001b[39;00m(\u001b[43mDimeNet\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The DimeNet++ from the `\"Fast and Uncertainty-Aware\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Directional Message Passing for Non-Equilibrium Molecules\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    <https://arxiv.org/abs/2011.14115>`_ paper.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m            (default: :obj:`\"swish\"`)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     url \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/gasteigerjo/dimenet/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaster/pretrained/dimenet_pp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DimeNet' is not defined"
     ]
    }
   ],
   "source": [
    "class DimeNetPlusPlus(DimeNet):\n",
    "    r\"\"\"The DimeNet++ from the `\"Fast and Uncertainty-Aware\n",
    "    Directional Message Passing for Non-Equilibrium Molecules\"\n",
    "    <https://arxiv.org/abs/2011.14115>`_ paper.\n",
    "    :class:`DimeNetPlusPlus` is an upgrade to the :class:`DimeNet` model with\n",
    "    8x faster and 10% more accurate than :class:`DimeNet`.\n",
    "    Args:\n",
    "        hidden_channels (int): Hidden embedding size.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_blocks (int): Number of building blocks.\n",
    "        int_emb_size (int): Size of embedding in the interaction block.\n",
    "        basis_emb_size (int): Size of basis embedding in the interaction block.\n",
    "        out_emb_channels (int): Size of embedding in the output block.\n",
    "        num_spherical (int): Number of spherical harmonics.\n",
    "        num_radial (int): Number of radial basis functions.\n",
    "        cutoff: (float, optional): Cutoff distance for interatomic\n",
    "            interactions. (default: :obj:`5.0`)\n",
    "        max_num_neighbors (int, optional): The maximum number of neighbors to\n",
    "            collect for each node within the :attr:`cutoff` distance.\n",
    "            (default: :obj:`32`)\n",
    "        envelope_exponent (int, optional): Shape of the smooth cutoff.\n",
    "            (default: :obj:`5`)\n",
    "        num_before_skip: (int, optional): Number of residual layers in the\n",
    "            interaction blocks before the skip connection. (default: :obj:`1`)\n",
    "        num_after_skip: (int, optional): Number of residual layers in the\n",
    "            interaction blocks after the skip connection. (default: :obj:`2`)\n",
    "        num_output_layers: (int, optional): Number of linear layers for the\n",
    "            output blocks. (default: :obj:`3`)\n",
    "        act: (str or Callable, optional): The activation funtion.\n",
    "            (default: :obj:`\"swish\"`)\n",
    "    \"\"\"\n",
    "\n",
    "    url = ('https://raw.githubusercontent.com/gasteigerjo/dimenet/'\n",
    "           'master/pretrained/dimenet_pp')\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        num_blocks: int,\n",
    "        int_emb_size: int,\n",
    "        basis_emb_size: int,\n",
    "        out_emb_channels: int,\n",
    "        num_spherical: int,\n",
    "        num_radial: int,\n",
    "        cutoff: float = 5.0,\n",
    "        max_num_neighbors: int = 32,\n",
    "        envelope_exponent: int = 5,\n",
    "        num_before_skip: int = 1,\n",
    "        num_after_skip: int = 2,\n",
    "        num_output_layers: int = 3,\n",
    "        act: Union[str, Callable] = 'swish',\n",
    "    ):\n",
    "        act = activation_resolver(act)\n",
    "\n",
    "        super().__init__(\n",
    "            hidden_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            num_blocks=num_blocks,\n",
    "            num_bilinear=1,\n",
    "            num_spherical=num_spherical,\n",
    "            num_radial=num_radial,\n",
    "            cutoff=cutoff,\n",
    "            max_num_neighbors=max_num_neighbors,\n",
    "            envelope_exponent=envelope_exponent,\n",
    "            num_before_skip=num_before_skip,\n",
    "            num_after_skip=num_after_skip,\n",
    "            num_output_layers=num_output_layers,\n",
    "            act=act,\n",
    "        )\n",
    "\n",
    "        # We are re-using the RBF, SBF and embedding layers of `DimeNet` and\n",
    "        # redefine output_block and interaction_block in DimeNet++.\n",
    "        # Hence, it is to be noted that in the above initalization, the\n",
    "        # variable `num_bilinear` does not have any purpose as it is used\n",
    "        # solely in the `OutputBlock` of DimeNet:\n",
    "        self.output_blocks = torch.nn.ModuleList([\n",
    "            OutputPPBlock(num_radial, hidden_channels, out_emb_channels,\n",
    "                          out_channels, num_output_layers, act)\n",
    "            for _ in range(num_blocks + 1)\n",
    "        ])\n",
    "\n",
    "        self.interaction_blocks = torch.nn.ModuleList([\n",
    "            InteractionPPBlock(hidden_channels, int_emb_size, basis_emb_size,\n",
    "                               num_spherical, num_radial, num_before_skip,\n",
    "                               num_after_skip, act) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    @classmethod\n",
    "    def from_qm9_pretrained(\n",
    "        cls,\n",
    "        root: str,\n",
    "        dataset: Dataset,\n",
    "        target: int,\n",
    "    ) -> Tuple['DimeNetPlusPlus', Dataset, Dataset, Dataset]:\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "        import tensorflow as tf\n",
    "\n",
    "        assert target >= 0 and target <= 12 and not target == 4\n",
    "\n",
    "        root = osp.expanduser(osp.normpath(root))\n",
    "        path = osp.join(root, 'pretrained_dimenet_pp', qm9_target_dict[target])\n",
    "\n",
    "        makedirs(path)\n",
    "        url = f'{cls.url}/{qm9_target_dict[target]}'\n",
    "\n",
    "        if not osp.exists(osp.join(path, 'checkpoint')):\n",
    "            download_url(f'{url}/checkpoint', path)\n",
    "            download_url(f'{url}/ckpt.data-00000-of-00002', path)\n",
    "            download_url(f'{url}/ckpt.data-00001-of-00002', path)\n",
    "            download_url(f'{url}/ckpt.index', path)\n",
    "\n",
    "        path = osp.join(path, 'ckpt')\n",
    "        reader = tf.train.load_checkpoint(path)\n",
    "\n",
    "        # Configuration from DimeNet++:\n",
    "        # https://github.com/gasteigerjo/dimenet/blob/master/config_pp.yaml\n",
    "        model = cls(\n",
    "            hidden_channels=128,\n",
    "            out_channels=1,\n",
    "            num_blocks=4,\n",
    "            int_emb_size=64,\n",
    "            basis_emb_size=8,\n",
    "            out_emb_channels=256,\n",
    "            num_spherical=7,\n",
    "            num_radial=6,\n",
    "            cutoff=5.0,\n",
    "            max_num_neighbors=32,\n",
    "            envelope_exponent=5,\n",
    "            num_before_skip=1,\n",
    "            num_after_skip=2,\n",
    "            num_output_layers=3,\n",
    "        )\n",
    "\n",
    "        def copy_(src, name, transpose=False):\n",
    "            init = reader.get_tensor(f'{name}/.ATTRIBUTES/VARIABLE_VALUE')\n",
    "            init = torch.from_numpy(init)\n",
    "            if name[-6:] == 'kernel':\n",
    "                init = init.t()\n",
    "            src.data.copy_(init)\n",
    "\n",
    "        copy_(model.rbf.freq, 'rbf_layer/frequencies')\n",
    "        copy_(model.emb.emb.weight, 'emb_block/embeddings')\n",
    "        copy_(model.emb.lin_rbf.weight, 'emb_block/dense_rbf/kernel')\n",
    "        copy_(model.emb.lin_rbf.bias, 'emb_block/dense_rbf/bias')\n",
    "        copy_(model.emb.lin.weight, 'emb_block/dense/kernel')\n",
    "        copy_(model.emb.lin.bias, 'emb_block/dense/bias')\n",
    "\n",
    "        for i, block in enumerate(model.output_blocks):\n",
    "            copy_(block.lin_rbf.weight, f'output_blocks/{i}/dense_rbf/kernel')\n",
    "            copy_(block.lin_up.weight,\n",
    "                  f'output_blocks/{i}/up_projection/kernel')\n",
    "            for j, lin in enumerate(block.lins):\n",
    "                copy_(lin.weight, f'output_blocks/{i}/dense_layers/{j}/kernel')\n",
    "                copy_(lin.bias, f'output_blocks/{i}/dense_layers/{j}/bias')\n",
    "            copy_(block.lin.weight, f'output_blocks/{i}/dense_final/kernel')\n",
    "\n",
    "        for i, block in enumerate(model.interaction_blocks):\n",
    "            copy_(block.lin_rbf1.weight, f'int_blocks/{i}/dense_rbf1/kernel')\n",
    "            copy_(block.lin_rbf2.weight, f'int_blocks/{i}/dense_rbf2/kernel')\n",
    "            copy_(block.lin_sbf1.weight, f'int_blocks/{i}/dense_sbf1/kernel')\n",
    "            copy_(block.lin_sbf2.weight, f'int_blocks/{i}/dense_sbf2/kernel')\n",
    "\n",
    "            copy_(block.lin_ji.weight, f'int_blocks/{i}/dense_ji/kernel')\n",
    "            copy_(block.lin_ji.bias, f'int_blocks/{i}/dense_ji/bias')\n",
    "            copy_(block.lin_kj.weight, f'int_blocks/{i}/dense_kj/kernel')\n",
    "            copy_(block.lin_kj.bias, f'int_blocks/{i}/dense_kj/bias')\n",
    "\n",
    "            copy_(block.lin_down.weight,\n",
    "                  f'int_blocks/{i}/down_projection/kernel')\n",
    "            copy_(block.lin_up.weight, f'int_blocks/{i}/up_projection/kernel')\n",
    "\n",
    "            for j, layer in enumerate(block.layers_before_skip):\n",
    "                copy_(layer.lin1.weight,\n",
    "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_1/kernel')\n",
    "                copy_(layer.lin1.bias,\n",
    "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_1/bias')\n",
    "                copy_(layer.lin2.weight,\n",
    "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_2/kernel')\n",
    "                copy_(layer.lin2.bias,\n",
    "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_2/bias')\n",
    "\n",
    "            copy_(block.lin.weight, f'int_blocks/{i}/final_before_skip/kernel')\n",
    "            copy_(block.lin.bias, f'int_blocks/{i}/final_before_skip/bias')\n",
    "\n",
    "            for j, layer in enumerate(block.layers_after_skip):\n",
    "                copy_(layer.lin1.weight,\n",
    "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_1/kernel')\n",
    "                copy_(layer.lin1.bias,\n",
    "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_1/bias')\n",
    "                copy_(layer.lin2.weight,\n",
    "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_2/kernel')\n",
    "                copy_(layer.lin2.bias,\n",
    "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_2/bias')\n",
    "\n",
    "        random_state = np.random.RandomState(seed=42)\n",
    "        perm = torch.from_numpy(random_state.permutation(np.arange(130831)))\n",
    "        train_idx = perm[:110000]\n",
    "        val_idx = perm[110000:120000]\n",
    "        test_idx = perm[120000:]\n",
    "\n",
    "        return model, (dataset[train_idx], dataset[val_idx], dataset[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86bc3a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_target_dict = {\n",
    "    0: 'mu',\n",
    "    1: 'alpha',\n",
    "    2: 'homo',\n",
    "    3: 'lumo',\n",
    "    5: 'r2',\n",
    "    6: 'zpve',\n",
    "    7: 'U0',\n",
    "    8: 'U',\n",
    "    9: 'H',\n",
    "    10: 'G',\n",
    "    11: 'Cv',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc743d0",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d1354",
   "metadata": {},
   "source": [
    "This blog is inspired from the following sources,\n",
    "- <a href=\"https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/nn/models/dimenet.py\">Pytorch Geometric Model</a>\n",
    "- The original <a href=\"https://arxiv.org/abs/2003.03123\">Directional Message Passing for Molecular Graphs</a> paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01df87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
