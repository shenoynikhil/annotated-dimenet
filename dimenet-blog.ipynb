{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01014d5c",
   "metadata": {},
   "source": [
    "## Annotated DimeNet [<a href=\"https://arxiv.org/abs/2003.03123\">Paper</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4565445",
   "metadata": {},
   "source": [
    "Molecular representation is a crucial task in computational chemistry and drug discovery. To understand the properties and interactions of molecules, it is essential to have the ability to accurately represent their complex structures. One approach that has shown promise in this area is the DimeNet architecture (https://arxiv.org/abs/2003.03123, <a href=\"https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/nn/models/dimenet.py\">Pytorch Source Code</a>). DimeNet utilizes directional message passing to efficiently and accurately represent molecules. In this blog, we will use PyTorch to explore each component of this network. Feel free to follow along in a Google Colab for hands-on experience.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Graph Neural Networks (GNNs) have become a highly sought-after architecture for modeling the quantum mechanical properties of molecules. In the past, GNNs primarily used 2D graph representations of molecules. However, recent advancements such as the DimeNet paper, which utilizes 3D graph representations, have greatly improved the performance of GNNs on these tasks. This paper proposes a new technique called directional message passing. This approach utilizes the positions of atoms in 3D and performs message passing using inter-atomic distances and angles between triplets of atoms, resulting in a more efficient and accurate representation of the molecules.\n",
    "\n",
    "The contributions of this paper are as follows:\n",
    "* A message passing scheme that utilizes directional information.\n",
    "* The construction of directional embeddings using Spherical Bessel functions and Spherical Harmonics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f9a8a",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "This blog assumes familiarity with message passing in graph neural networks and the usage of PyTorch. For people new to it, feel free to refer to these excellent distil pub articles,\n",
    "- https://distill.pub/2021/gnn-intro/\n",
    "- https://distill.pub/2021/understanding-gnns/\n",
    "- [Scatter Function](https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html#torch_scatter.scatter) for implementing message passing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d8bbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Overview\n",
    "The blog is structured as follows:\n",
    "\n",
    "1. An explanation of how to construct directional embeddings using 2D spherical Fourier-Bessel Basis Functions.\n",
    "2. A detailed description of the architecture of DimeNet and the implementation of each component.\n",
    "3. An experiment to demonstrate the use of DimeNet architecture on QM9 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe775950",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Package Installation Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04a03e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure there is pyg, torch_geometric, torch, sympy, jupyter\n",
    "# conda create -n annotated-dimenet python=3.8\n",
    "# conda activate annotated-dimenet\n",
    "# conda install pytorch torchvision pytorch-cuda=11.6 -c pytorch -c nvidia\n",
    "# pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
    "# pip install torch_geometric\n",
    "# conda install sympy\n",
    "# pip install jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2df8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06df215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/arc/project/st-jiaruid-1/miniconda3/envs/graph/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from math import pi as PI\n",
    "from math import sqrt\n",
    "from typing import Callable, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, Linear\n",
    "from torch_scatter import scatter\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "from torch_geometric.data import Dataset, download_url\n",
    "from torch_geometric.data.makedirs import makedirs\n",
    "from torch_geometric.nn import radius_graph\n",
    "from torch_geometric.nn.inits import glorot_orthogonal\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.typing import OptTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d94223-d628-409d-a0aa-45f7ad649c81",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Directional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c952e7b-47ae-4da7-a882-59141ae63ab9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"imgs/message-passing.png\" width=\"200\"/> <br/>\n",
    "</center>\n",
    "\n",
    "The key idea in this paper is to include directional information during message passing. Consider an edge between atom `i` and `j`, the message from `j` to `i` is represented as $m_{ji}$. The message $m_{ji}$ is updated with the help of directional information by considering neighbours of `j` forming an atom triplet `ijk`. For example, we can use the angle between the direction `kj` (from atom `k` to atom `j`) and direction `ji` (from atom `j` to atom `i`) using $\\alpha_{(kj, ji)} = \\angle x_k x_j x_i$.  To update the directional embedding $m_{ji}$, we first consider messages $m_{kj}$ from all the neighbours (as seen in the figure below) to `j`. The message passing update for edge embeddings can be updated from the following three,\n",
    "* incoming messages $m_{kj}$ from neighbours `k`\n",
    "* the directional information $\\alpha_{(kj, ji)}$ \n",
    "* the inter-atomic distance $d_{ij}$. \n",
    "\n",
    "$$\n",
    "h_i = \\sum_{k \\in \\mathcal{N}_i}m_{ki}\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bd9e3-6d7e-4be2-becd-58b21f50f217",
   "metadata": {},
   "source": [
    "### Representation of inter-atomic distances and angles\n",
    "As we previously discussed, the message passing layer involves inter-atomic distances and angles. A logical question that arises is how these interatomic distances and angles are provided to the model. The paper employs the following two methods to transform the raw values:\n",
    "\n",
    "* <i>Radial Basis Function (RBF)</i> $e_{RBF}^{{ji}}$ for interatomic distances $d_{ji}$.\n",
    "* <i>Spherical Basis Function (SBF)</i> $a_{SBF}^{(kj, ji)}$, a joint angle and distance-based function that takes as input the angle $\\alpha_{(kj, ji)}$ and the interatomic distance $d_{kj}$.\n",
    "\n",
    "<i>Think of the above two functions as ways to transform the input (angles and distances) into a form or space where the model can more easily extract relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17f84b-b825-4366-b55b-bac784354345",
   "metadata": {},
   "source": [
    "### Radial Basis Function\n",
    "The radial basis function used here is an orthogonal basis. It takes into input the interatomic distance $d$ and a cutoff distance $c$. The functional form is as follows, \n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{e}}_{RBF}(d) = \\sqrt{\\frac{2}{c}}\\frac{\\sin{(\\frac{n\\pi}{c}}d)}{d}\n",
    "$$\n",
    "\n",
    "* The value of the integer $n$ ranges from $n \\in [1, ... , N_{RBF}]$, where $N_{RBF}$ denotes the number of orthogonal components (bases).\n",
    "* To simplify, each inter-atomic distance input $d$ will now be converted into a $N_{RBF}$ sized tensor using the radial basis function.\n",
    "* Using this basis improves parameter efficiency by using 1/4th of the number of parameters used by a Gaussian RBF (Table 3 in the [Paper](https://arxiv.org/abs/2003.03123))\n",
    "<!-- * NOT INCLUDING THIS The paper verifies that this basis function requires 1/4th the number of parameters ($N_{RBF}$) as compared to the gaussian radial basis -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8a2a9-8b31-43fa-93b6-73bb61de66db",
   "metadata": {},
   "source": [
    "### Spherical Basis Function\n",
    "This basis layer is a joint 2D basis for $d_{kj}$ and $\\alpha_{(kj,ji)}$, a function that depends on both the interatomic distance and an angle. The functional form is as follows,\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{a}}_{SBF, ln}(d, \\alpha) = \\sqrt{\\frac{2}{c^3 j^2_{l + 1}(z_{ln})}} j_l(\\frac{z_{ln}}{c}d)Y_l^0(\\alpha)\n",
    "$$\n",
    "Here, $l \\in [0 .. N_{SHBF} - 1]$ and $n \\in [1 ... N_{SRBF}]$. The $z_{ln}$ can be computed using bessel function implementations, while $Y_l^0(\\alpha)$ can be computed using spherical harmonics function implementations.\n",
    "\n",
    "<i>An easier way to think about this is that each pair of interatomic distance input $d_{ij}$ and angle $\\alpha_{(kj,ji)}$ is converted into a $N_{SRBF} \\times N_{SHBF}$ sized tensor using the spherical basis function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652fb56c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Continuous Cutoff using an Envelope Function\n",
    "\n",
    "As we use the 3D structure of the molecule in the model, every interatomic pair could potentially be considered for message passing. However, using all interatomic pairs could significantly increase the computational overhead. To mitigate this, we introduce a cutoff ($c$) on the interatomic distance to determine neighbors.\n",
    "\n",
    "One issue that arises with a hard cutoff is that the above functions (radial basis and spherical basis) that depend on $d$ are no longer twice continuously differentiable. To alleviate this problem, the paper multiplies the $\\tilde{\\mathcal{a}}_{SBF}(d)$ and $\\tilde{\\mathcal{e}}_{RBF}(d)$ with an envelope function $u(d)$, which makes the function twice differentiable.\n",
    "\n",
    "$$\n",
    "\\mathcal{e}_{RBF}(d) = u(d)\\tilde{\\mathcal{e}}_{RBF}(d)\n",
    "$$\n",
    "$$\n",
    "\\mathcal{a}_{SBF}(d) = u(d) \\tilde{\\mathcal{a}}_{SBF}(d)\n",
    "$$\n",
    "\n",
    "Here, $u(d)$ is the envelope function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b556e056-c1ee-44b8-ae75-af188c98670b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Message Passing Equation\n",
    "The message passing equation would be as follows,\n",
    "\n",
    "$$\n",
    "m_{ji}^{(l + 1)} = f_{\\text{update}}(m_{ji}^{(l)}, \\sum_{k in \\mathcal{N}_j \\backslash \\{i\\}}f_{\\text{int}}(m_{kj}^{(l)}), e_{\\text{RBF}}^{(ji)}, a_{\\text{SBF}}^{(kj, ji)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1fe56d-a50f-495e-968b-68d95027740f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implementation of Spherical Basis Function and Radial Basis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3b970-220c-4cd5-a4ed-ef7e8bca71ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Continuous Cutoff Envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434dae3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "The equation for the continuous envelope\n",
    "\n",
    "$$\n",
    "u(d) = 1 - \\frac{(p + 1)(p + 2)}{2}d^p + p(p + 2)d^{p+1} - \\frac{p(p + 1)}{2}d^{p + 2}\n",
    "$$\n",
    "\n",
    "The paper uses a default value of p = 6 or `exponent = 5.` in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f5e8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Envelope(torch.nn.Module):\n",
    "    def __init__(self, exponent: int):\n",
    "        super().__init__()\n",
    "        self.p = exponent + 1\n",
    "        self.a = -(self.p + 1) * (self.p + 2) / 2\n",
    "        self.b = self.p * (self.p + 2)\n",
    "        self.c = -self.p * (self.p + 1) / 2\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        p, a, b, c = self.p, self.a, self.b, self.c\n",
    "        x_pow_p0 = x.pow(p - 1)\n",
    "        x_pow_p1 = x_pow_p0 * x\n",
    "        x_pow_p2 = x_pow_p1 * x\n",
    "        \n",
    "        return (\n",
    "            1. / x + a * x_pow_p0 + b * x_pow_p1 +\n",
    "            c * x_pow_p2) * (x < 1.0).to(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6bed16-0f23-440c-ad98-92f26e228614",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Radial Basis Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db48ec4a",
   "metadata": {},
   "source": [
    "If we recall the equation,\n",
    "$$\n",
    "\\mathcal{e}_{RBF}(d) = u(d)\\tilde{\\mathcal{e}}_{RBF}(d)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{e}}_{RBF}(d) = \\sqrt{\\frac{2}{c}}\\frac{\\sin{(\\frac{n\\pi}{c}}d)}{d} \n",
    "$$\n",
    "\n",
    "$$\n",
    "u(d) : \\text{Envelope Equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "n \\in [1, ... , N_{RBF}]\n",
    "$$\n",
    "\n",
    "where $N_{RBF}$ denotes the number of orthogonal bases. The paper implements the Radial Basis Function as a layer where the frequency information is learned via backpropagation. We initialize the parameter values initially as $n\\pi/c$. The following mapping from the math variables to code variables is used,\n",
    "- $N_{RBF} \\to $ `num_radial` \n",
    "- $c \\to $ `cutoff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdc1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBasisLayer(torch.nn.Module):\n",
    "    '''RadialBasisLayer'''\n",
    "    def __init__(self, num_radial: int, cutoff: float = 5.0,\n",
    "                 envelope_exponent: int = 5):\n",
    "        super().__init__()\n",
    "        # the c in the radial basis layer equation\n",
    "        self.cutoff = cutoff\n",
    "        # u(d) / envelope\n",
    "        self.envelope = Envelope(envelope_exponent)\n",
    "        \n",
    "        # the different frequencies to be considered to generate orthogonal basis\n",
    "        self.freq = torch.nn.Parameter(torch.Tensor(num_radial))\n",
    "        \n",
    "        # make sure we reset_parameters during __init__()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        with torch.no_grad():\n",
    "            torch.arange(1, self.freq.numel() + 1, out=self.freq).mul_(PI)\n",
    "        self.freq.requires_grad_()\n",
    "\n",
    "    def forward(self, dist: Tensor) -> Tensor:\n",
    "        # compute d = (d/c)\n",
    "        dist = (dist.unsqueeze(-1) / self.cutoff)\n",
    "        # compute u(d/c) * sin(\\frac{n\\pi}{c} \\times d)\n",
    "        return self.envelope(dist) * (self.freq * dist).sin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518bd40d-c32d-4dfe-a840-669ce54723d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Spherical Basis Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c5b16",
   "metadata": {},
   "source": [
    "If we recall the equation,\n",
    "\n",
    "$$\n",
    "\\mathcal{a}_{RBF}(d) = u(d) \\tilde{\\mathcal{a}}_{RBF}(d) \\\\\n",
    "$$\n",
    "$$\n",
    "\\tilde{\\mathcal{a}}_{SBF, ln}(d, \\alpha) = \\sqrt{\\frac{2}{c^3 j^2_{l + 1}(z_{ln})}} j_l(\\frac{z_{ln}}{c}d)Y_l^0(\\alpha)\n",
    "$$\n",
    "where $l \\in [0 .. N_{SHBF} - 1]$ and $n \\in [1 ... N_{SRBF}]$\n",
    "\n",
    "The following mapping from math variables to code variables will be used,\n",
    "* $N_{SHBF} \\to $ `num_spherical` \n",
    "* $N_{SRBF} \\to $ `num_radial` \n",
    "* $c \\to $ `cutoff`\n",
    "\n",
    "The following two variables represent,\n",
    "* $z_{ln}$: $n'$th root of the $l$-order Bessel Functions\n",
    "* $Y_l^0(\\alpha)$: Special Harmonics\n",
    "\n",
    "These variables have been implemented in `torch_geometric.nn.models.dimenet_utils` as `bessel_basis` and `real_sph_harm`. The exact details of implementing these functions using sympy's <a href='https://docs.sympy.org/latest/index.html'>symbolic computation library</a> are beyond the scope of this blog. However, it's worth noting that,\n",
    "- $z_{ln}$ has a total of $N_{SHBF} \\times N_{SRBF}$ values as it varies with both $l$ and $n$ whereas $Y_l^0(\\alpha)$ has $N_{SHBF}$ values as it varies with $l$\n",
    "- The output of these functions are sympy expressions, which means they will contain symbols like `sin()`, `cos()`, and `x`\n",
    "- To convert any symbolic expression involving (`x`, `sin` and `cos`) into a lambda function, we use `sym.lambdify([x], expression, modules)` [<a href='https://docs.sympy.org/latest/modules/utilities/lambdify.html#sympy.utilities.lambdify.lambdify'>Link</a>], the modules here maps the symbolic functions `sin()`and `cos()` to our torch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8d1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "\n",
    "from torch_geometric.nn.models.dimenet_utils import (\n",
    "    bessel_basis, # SRBF\n",
    "    real_sph_harm, # SHBF\n",
    ")\n",
    "\n",
    "class SphericalBasisLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_spherical: int, \n",
    "        num_radial: int,\n",
    "        cutoff: float = 5.0, \n",
    "        envelope_exponent: int = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_radial <= 64\n",
    "        self.num_spherical = num_spherical\n",
    "        self.num_radial = num_radial\n",
    "        self.cutoff = cutoff\n",
    "        self.envelope = Envelope(envelope_exponent)\n",
    "        \n",
    "        # We can bessel basis and spherical Harmonic forms in sympy expressions\n",
    "        # sympy expressions -> Equations containing `sin` `cos` `x` and `theta`\n",
    "        # computing z_{ln} constant, based on l(num_spherical) and n(num_radial)\n",
    "        bessel_forms = bessel_basis(num_spherical, num_radial)\n",
    "        # computing Y_l^0(\\alpha)\n",
    "        sph_harm_forms = real_sph_harm(num_spherical)\n",
    "        \n",
    "        # let's fill spherical and radial functions for \n",
    "        # l \\in [0, ..., N_SHBF - 1]\n",
    "        # n \\in [1, ..., N_SRBF]\n",
    "        \n",
    "        # spherical functions only dependent on l, there will be N_SHBF of them\n",
    "        self.sph_funcs = []\n",
    "        # bessel functions dependent on l and n, there will be N_SHBF x N_SRBF of them\n",
    "        self.bessel_funcs = []\n",
    "            \n",
    "        # Using Sympy, we convert sympy expressions into lambda functions\n",
    "        x, theta = sym.symbols('x theta')\n",
    "        modules = {'sin': torch.sin, 'cos': torch.cos}\n",
    "        \n",
    "        # i goes from 0 to num_spherical x-x 1 (exactly the range of l)\n",
    "        for i in range(num_spherical):\n",
    "            if i == 0:\n",
    "                sph1 = sym.lambdify([theta], sph_harm_forms[i][0], modules)(0)\n",
    "                self.sph_funcs.append(lambda x: torch.zeros_like(x) + sph1)\n",
    "            else:\n",
    "                sph = sym.lambdify([theta], sph_harm_forms[i][0], modules)\n",
    "                self.sph_funcs.append(sph)\n",
    "                \n",
    "            for j in range(num_radial):\n",
    "                bessel = sym.lambdify([x], bessel_forms[i][j], modules)\n",
    "                self.bessel_funcs.append(bessel)\n",
    "\n",
    "    def forward(self, dist: Tensor, angle: Tensor, idx_kj: Tensor) -> Tensor:\n",
    "        '''Performs Forward Pass'''\n",
    "        # computes d / c\n",
    "        dist = dist / self.cutoff\n",
    "        \n",
    "        # n, k = self.num_spherical, self.num_radial\n",
    "        # computes over all radial bessel (n x k) functions and stack over feature dimension (1)\n",
    "        rbf = torch.stack([f(dist) for f in self.bessel_funcs], dim=1)\n",
    "        # multiply with u(d) * bessel() \n",
    "        # Since u(d) will be of same shape as d, u(d).unsqueeze(-1) to allow dot product\n",
    "        rbf = self.envelope(dist).unsqueeze(-1) * rbf\n",
    "        \n",
    "        # compute over all spherical (n) functions and stack over feature dimension (1)\n",
    "        sbf = torch.stack([f(angle) for f in self.sph_funcs], dim=1)\n",
    "        \n",
    "        n, k = self.num_spherical, self.num_radial\n",
    "        # multiply the two to get a_SBF = u(d) * radial_bessel() * spherical_harmonics()\n",
    "        out = (rbf[idx_kj].view(-1, n, k) * sbf.view(-1, n, 1)).view(-1, n * k)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87712b26-691c-4962-bf83-8d99ec345af2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Architecture of DimeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67d628-bb0f-477e-bd68-6161679b90ce",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='imgs/dimenet-arch.png' width='600'/>\n",
    "</center>\n",
    "\n",
    "The architecture consists of the following,\n",
    "- RBF and SBF to transform our interatomic distances and angles\n",
    "- Embedding Blocks to Initialize initial messages embeddings\n",
    "- (Multiple) Interaction Blocks to perform Message Passing\n",
    "- Output Block to convert embeddings to generate a prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde5b0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<b> High-Level Overview of Network Architecture</b><br>\n",
    "Let's take a look at the high-level structure of the architecture and informally write down the `forward()` function. The architecture is as follows:\n",
    "<center>\n",
    "<img src=\"imgs/model-only.png\" width=\"200\"/>\n",
    "</center>\n",
    "\n",
    "* Given the number of nodes `num_nodes` and edge_index `edge_index`, get all triplets of form `ijk` and edge indices of `ji` and `kj`\n",
    "* From each triplet, we can get the distance of the `ij` pair and the angle `ijk` with the help of the coordinates,\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/angle-distance.png\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "* Convert distances and angles into a tensor\n",
    "* Transform the inter-atomic distance using the radial basis function `self.rbf(d)`\n",
    "* Transform the inter-atomic distance and angle using the spherical basis function `self.sbf(d, angle)`\n",
    "* The outputs are constructed from `1 Embedding Block` and `6 Interaction Blocks` in a sequential manner, where:\n",
    "    * The first output prediction comes from the `embedding + output` block using initial input embeddings\n",
    "    * All other output predictions come from the `interaction + output` block using embeddings from the previous layer\n",
    "* All the outputs are summed up to provide the final scalar prediction per molecule\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844d981-e44e-4032-8e99-a17ec133af9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Embedding Block: Getting Message Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4853b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's take a closer look at the Embedding Block.\n",
    "\n",
    "<center>\n",
    "<img src='imgs/embedding-block.png' width='400'/>\n",
    "</center>\n",
    "\n",
    "Here's what happens in this block:\n",
    "1. Four inputs are passed to this Embedding Block (as seen in the `forward()` function)\n",
    "    - The atomic Numbers of the atoms in the batch (`x`)\n",
    "    - The distance between atoms `i` and `j`, transformed using the Radial Basis Function (RBF)\n",
    "    - The atomic number of `i`\n",
    "    - The atomic number of `j`\n",
    "2. The RBF output is passed through a `Linear(num_radial, hidden_channels)` layer to obtain distance embeddings.\n",
    "3. Atomic numbers are transformed into learnable embeddings using `nn.Embedding(95, hidden_channels)`. Here, 95 is the maximum atomic number that can be expected.\n",
    "3. The embeddings for atoms `i` and `j`, and the distance embeddings are concatenated along the `dim=1` dimension. This results in a shape `3 x hidden_channels`.\n",
    "4. A `Linear(3 * hidden_channels, hidden_channels)` layer is applied to the concatenated representation, resulting in the final edge embedding between atoms `i` and `j`.\n",
    "5. The final message, along with the RBF input, is passed to the `Output Block` to obtain a scalar prediction for each atom $t_i^{(1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a30d1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(torch.nn.Module):\n",
    "    '''Implementation of Embedding Block\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_radial: int\n",
    "        Number of radial features (feature dimensions of rbf output)\n",
    "    hidden_channels: int\n",
    "        feature dimension of output of this embedding block\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, num_radial: int, hidden_channels: int, act: Callable):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        \n",
    "        self.emb = Embedding(95, hidden_channels)\n",
    "        self.lin_rbf = Linear(num_radial, hidden_channels)\n",
    "        self.lin = Linear(3 * hidden_channels, hidden_channels)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.emb.weight.data.uniform_(-sqrt(3), sqrt(3))\n",
    "        self.lin_rbf.reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor, j: Tensor) -> Tensor:\n",
    "        x = self.emb(x)\n",
    "        rbf = self.act(self.lin_rbf(rbf))\n",
    "        return self.act(self.lin(torch.cat([x[i], x[j], rbf], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d960a54-e664-4e50-b0fa-fdd6508df2ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Residual Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a13e3-8529-45df-9095-ac12950d18fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Interaction Block makes use of residual layers, inspired by ResNets, to improve the flow of gradients. The architecture of a residual layer is as follows:\n",
    "\n",
    "<center>\n",
    "<img src='imgs/residual.png' width='200'/>\n",
    "</center>\n",
    "\n",
    "The implementation is as follows,\n",
    "- 2 `nn.Linear` layers are used, which do not change the input feature dimension\n",
    "- A non-linear activation function is applied between the two linear layers, and the initial input (`x`) is added to the output `act(lin_2(act(lin_1(x))))` to produce the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73e8580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels: int, act: Callable):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot_orthogonal(self.lin1.weight, scale=2.0)\n",
    "        self.lin1.bias.data.fill_(0)\n",
    "        glorot_orthogonal(self.lin2.weight, scale=2.0)\n",
    "        self.lin2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x + self.act(self.lin2(self.act(self.lin1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079aa4a0-136d-42c1-8fd8-49d0327d7b9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Interaction Block: Message Passing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bc26b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "This block implements the main message passing equation,\n",
    "\n",
    "$$\n",
    "m_{ji}^{(l + 1)} = f_{\\text{update}}(m_{ji}^{(l)}, \\sum_{k in \\mathcal{N}_j \\backslash \\{i\\}}f_{\\text{int}}(m_{kj}^{(l)}), e_{\\text{RBF}}^{(ji)}, a_{\\text{SBF}}^{(kj, ji)})\n",
    "$$\n",
    "\n",
    "<center>\n",
    "    <img src='imgs/interaction-block.png' width='500'/>\n",
    "</center>\n",
    "\n",
    "\n",
    "Some points over the implementation of the interaction block,\n",
    "1. The inputs to this block are,\n",
    "    - Input embedding features `x` (from the embedding/interaction block before)\n",
    "    - Radial Basis Function input (input feature dimension: `num_radial`)\n",
    "    - Spherical Basis Function embeddings (input feature dimension: `num_spherical * num_radial`)\n",
    "    - `idx_kj`: All indices from atom `k` to atom `j`\n",
    "    - `idx_ji`: All indices from atom `j` to atom `i`\n",
    "2. Apply `Linear(num_radial, hidden_channels, bias=False)` to $e_{\\text{RBF}}$\n",
    "3. $a_{\\text{SBF}}$ is transformed into a $N_\\text{bilinear}$-representation using `Linear(num_radial * num_spherical, num_bilinear, bias=False)`\n",
    "3. To compute the message to be calculated,\n",
    "    - Transform the messages $x_{ij}$ and $x_{kj}$ using `activation(linear(hidden_channels, hidden_channels))`\n",
    "    - Dot product between `linear(rbf)` and above transformed $x_{kj}$\n",
    "    - Bilinear product $a_{sbf}^T\\cdot W \\cdot x_{kj}$\n",
    "    - Perform message passing using `torch_scatter.scatter` to get $\\sum_k {m_{kj}}$\n",
    "4. Post Message Passing, update our $m_{ji}^{(l)} = \\text{Swish Activation}(\\text{linear}(m_{ji}^{(l - 1)})) + \\sum_k {m_{kj}}$ \n",
    "5. Run this through some `Residual Layer`\n",
    "6. Pass the final updated message along with RBF input into the `Output Layer` to get the target scalar value for that interaction block $l$ and each atom $i$ as $t_i^{(l)}$\n",
    "\n",
    "<b>Note</b>: [Swish](https://arxiv.org/abs/1710.05941v2) / [SiLU](https://arxiv.org/abs/1702.03118) works much better than activation functions used in previous works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e401ac9-41d6-49d5-a2fb-e79e4d2eccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionBlock(torch.nn.Module):\n",
    "    '''Interaction Block in DimeNet: Reponsible for Message Passing'''\n",
    "    def __init__(self, hidden_channels: int, num_bilinear: int,\n",
    "                 num_spherical: int, num_radial: int, num_before_skip: int,\n",
    "                 num_after_skip: int, act: Callable):\n",
    "        '''\n",
    "        Initialize Interaction Module\n",
    "        \n",
    "        Args:\n",
    "            hidden_channels (int): Hidden embedding size.\n",
    "            num_bilinear (int): Size of the bilinear layer tensor.\n",
    "            num_spherical (int): Number of spherical harmonics.\n",
    "            num_radial (int): Number of radial basis functions.\n",
    "            num_before_skip (int, optional): Number of residual layers in the\n",
    "                interaction blocks before the skip connection. (default: :obj:`1`)\n",
    "            num_after_skip (int, optional): Number of residual layers in the\n",
    "                interaction blocks after the skip connection. (default: :obj:`2`)\n",
    "            act (str or Callable, optional): The activation function.\n",
    "                (default: :obj:`\"swish\"`)        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "\n",
    "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
    "        self.lin_sbf = Linear(num_spherical * num_radial, num_bilinear,\n",
    "                              bias=False)\n",
    "\n",
    "        # Dense transformations of input messages.\n",
    "        self.lin_kj = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin_ji = Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.W = torch.nn.Parameter(\n",
    "            torch.Tensor(hidden_channels, num_bilinear, hidden_channels))\n",
    "\n",
    "        self.layers_before_skip = torch.nn.ModuleList([\n",
    "            ResidualLayer(hidden_channels, act) for _ in range(num_before_skip)\n",
    "        ])\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.layers_after_skip = torch.nn.ModuleList([\n",
    "            ResidualLayer(hidden_channels, act) for _ in range(num_after_skip)\n",
    "        ])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        '''Initializing Parameters'''\n",
    "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_sbf.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_kj.weight, scale=2.0)\n",
    "        self.lin_kj.bias.data.fill_(0)\n",
    "        glorot_orthogonal(self.lin_ji.weight, scale=2.0)\n",
    "        self.lin_ji.bias.data.fill_(0)\n",
    "        self.W.data.normal_(mean=0, std=2 / self.W.size(0))\n",
    "        for res_layer in self.layers_before_skip:\n",
    "            res_layer.reset_parameters()\n",
    "        glorot_orthogonal(self.lin.weight, scale=2.0)\n",
    "        self.lin.bias.data.fill_(0)\n",
    "        for res_layer in self.layers_after_skip:\n",
    "            res_layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Tensor, rbf: Tensor, sbf: Tensor, idx_kj: Tensor,\n",
    "                idx_ji: Tensor) -> Tensor:\n",
    "        # transform rbf and sbf input using their respective nn.Linear()\n",
    "        rbf = self.lin_rbf(rbf)\n",
    "        sbf = self.lin_sbf(sbf)\n",
    "        \n",
    "        # Transform the messages into activation(linear(message))\n",
    "        x_ji = self.act(self.lin_ji(x))\n",
    "        x_kj = self.act(self.lin_kj(x))\n",
    "        x_kj = x_kj * rbf\n",
    "        \n",
    "        # bilinear product\n",
    "        x_kj = torch.einsum('wj,wl,ijl->wi', sbf, x_kj[idx_kj], self.W)\n",
    "        \n",
    "        # message passing\n",
    "        x_kj = scatter(x_kj, idx_ji, dim=0, dim_size=x.size(0))\n",
    "        \n",
    "        # update our message\n",
    "        h = x_ji + x_kj\n",
    "        \n",
    "        # Apply residual layers\n",
    "        for layer in self.layers_before_skip:\n",
    "            h = layer(h)\n",
    "        h = self.act(self.lin(h)) + x\n",
    "        for layer in self.layers_after_skip:\n",
    "            h = layer(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a16ba-0f22-4c23-9104-cd3b34bf914b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Output Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63770bd2",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='imgs/output-block.png' width='100'/>\n",
    "</center>\n",
    "\n",
    "The output block is applied after the embedding block and each interaction block. This is responsible for calculating the final scalar target $t_i^{(l)}$ for each atom ($i$) for that interaction block ($l$) or Embedding Block. The following steps happen:\n",
    "\n",
    "* Take in the RBF input and the input messages from the previous block as input\n",
    "* Calculate the dot product between the Linear Weight transformed RBF input and input messages. Using the torch_scatter.scatter() function is an efficient way to do this for each atom $i$.\n",
    "* Forward pass through standard feedforward neural network layers\n",
    "\n",
    "<i>It is important to note that the dot product between the transformed RBF output and input messages guarantees a twice differentiable output because of the envelope function.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab933f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputBlock(torch.nn.Module):\n",
    "    def __init__(self, num_radial: int, hidden_channels: int,\n",
    "                 out_channels: int, num_layers: int, act: Callable):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        \n",
    "        # Linear layer to convert rbf input\n",
    "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
    "        \n",
    "        # linear layers to convert output through num_layers\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.lins.append(Linear(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # final linear layer to convert it into `out_channels` dim output\n",
    "        self.lin = Linear(hidden_channels, out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
    "        for lin in self.lins:\n",
    "            glorot_orthogonal(lin.weight, scale=2.0)\n",
    "            lin.bias.data.fill_(0)\n",
    "        self.lin.weight.data.fill_(0)\n",
    "\n",
    "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor,\n",
    "                num_nodes: Optional[int] = None) -> Tensor:\n",
    "        x = self.lin_rbf(rbf) * x\n",
    "\n",
    "        # scatter-add to add messages from all m_{ji} to atom $i$\n",
    "        x = scatter(x, i, dim=0, dim_size=num_nodes)\n",
    "        \n",
    "        # pass the x through multiple linear layers\n",
    "        for lin in self.lins:\n",
    "            x = self.act(lin(x))\n",
    "        \n",
    "        # pass it through the final output layer\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11405f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Final Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4806de75-e456-4696-86f3-b0fec2cc2f91",
   "metadata": {},
   "source": [
    "The final prediction is calculated by summing up the outputs from the output blocks of all the interaction layers and the embedding layer.\n",
    "\n",
    "\n",
    "$$\n",
    "t = \\sum_i \\sum_l t_{i}^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd5fbd-d817-41e3-8349-b338a7302e27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### DimeNet: Putting it All Together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee350b",
   "metadata": {
    "tags": []
   },
   "source": [
    "A quick summary of the forward pass steps,\n",
    "* Given the 3D structure of the molecule, calculate the inter-atomic distance and angle for each atom triplet.\n",
    "* Use the radial basis function `self.rbf(d)` and spherical basis function `self.sbf(d, angle)` to transform these distances and angles.\n",
    "* Utilize the output from the embedding and interaction blocks, in a sequential manner, to construct the final prediction.\n",
    "* Sum the outputs to get a final scalar prediction per molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be2de2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimeNet(torch.nn.Module):\n",
    "    r\"\"\"The directional message passing neural network (DimeNet) from the\n",
    "    `\"Directional Message Passing for Molecular Graphs\"\n",
    "    <https://arxiv.org/abs/2003.03123>`_ paper.\n",
    "    DimeNet transforms messages based on the angle between them in a\n",
    "    rotation-equivariant fashion.\n",
    "\n",
    "    Args:\n",
    "        hidden_channels (int): Hidden embedding size.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_blocks (int): Number of building blocks.\n",
    "        num_bilinear (int): Size of the bilinear layer tensor.\n",
    "        num_spherical (int): Number of spherical harmonics.\n",
    "        num_radial (int): Number of radial basis functions.\n",
    "        cutoff (float, optional): Cutoff distance for interatomic\n",
    "            interactions. (default: :obj:`5.0`)\n",
    "        max_num_neighbors (int, optional): The maximum number of neighbors to\n",
    "            collect for each node within the :attr:`cutoff` distance.\n",
    "            (default: :obj:`32`)\n",
    "        envelope_exponent (int, optional): Shape of the smooth cutoff.\n",
    "            (default: :obj:`5`)\n",
    "        num_before_skip (int, optional): Number of residual layers in the\n",
    "            interaction blocks before the skip connection. (default: :obj:`1`)\n",
    "        num_after_skip (int, optional): Number of residual layers in the\n",
    "            interaction blocks after the skip connection. (default: :obj:`2`)\n",
    "        num_output_layers (int, optional): Number of linear layers for the\n",
    "            output blocks. (default: :obj:`3`)\n",
    "        act (str or Callable, optional): The activation function.\n",
    "            (default: :obj:`\"swish\"`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        num_blocks: int,\n",
    "        num_bilinear: int,\n",
    "        num_spherical: int,\n",
    "        num_radial,\n",
    "        cutoff: float = 5.0,\n",
    "        max_num_neighbors: int = 32,\n",
    "        envelope_exponent: int = 5,\n",
    "        num_before_skip: int = 1,\n",
    "        num_after_skip: int = 2,\n",
    "        num_output_layers: int = 3,\n",
    "        act: Union[str, Callable] = 'swish',\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_spherical < 2:\n",
    "            raise ValueError(\"num_spherical should be greater than 1\")\n",
    "        \n",
    "        # get activation function for the `act` string passing in `__init__`\n",
    "        act = activation_resolver(act)\n",
    "\n",
    "        # cutoff value $c$ below which we consider atoms to be neighbours\n",
    "        self.cutoff = cutoff\n",
    "        # If neighbours exceed this we consider top \n",
    "        # max_num_neighbours based on inter-atomic distance\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        \n",
    "        # number of interaction blocks/message passing blocks\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        # Our radial basis layer for inter-atomic distance\n",
    "        self.rbf = RadialBasisLayer(num_radial, cutoff, envelope_exponent)\n",
    "\n",
    "        # Spherical Basis Layer for 2D joint representation\n",
    "        # using distance and angle\n",
    "        self.sbf = SphericalBasisLayer(num_spherical, num_radial, cutoff,\n",
    "                                       envelope_exponent)\n",
    "        \n",
    "        # embedding block\n",
    "        self.emb = EmbeddingBlock(num_radial, hidden_channels, act)\n",
    "        \n",
    "        # embedding blocks\n",
    "        self.output_blocks = torch.nn.ModuleList([\n",
    "            OutputBlock(num_radial, hidden_channels, out_channels,\n",
    "                        num_output_layers, act) for _ in range(num_blocks + 1)\n",
    "        ])\n",
    "\n",
    "        self.interaction_blocks = torch.nn.ModuleList([\n",
    "            InteractionBlock(hidden_channels, num_bilinear, num_spherical,\n",
    "                             num_radial, num_before_skip, num_after_skip, act)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.rbf.reset_parameters()\n",
    "        self.emb.reset_parameters()\n",
    "        for out in self.output_blocks:\n",
    "            out.reset_parameters()\n",
    "        for interaction in self.interaction_blocks:\n",
    "            interaction.reset_parameters()\n",
    "\n",
    "    def triplets(\n",
    "        self,\n",
    "        edge_index: Tensor,\n",
    "        num_nodes: int,\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
    "        '''Get kji triplets for directional message passing'''\n",
    "        row, col = edge_index  # j->i\n",
    "        \n",
    "        value = torch.arange(row.size(0), device=row.device)\n",
    "        \n",
    "        # get sparse adjacency matrix\n",
    "        adj_t = SparseTensor(row=col, col=row, value=value,\n",
    "                             sparse_sizes=(num_nodes, num_nodes))\n",
    "        adj_t_row = adj_t[row]\n",
    "        num_triplets = adj_t_row.set_value(None).sum(dim=1).to(torch.long)\n",
    "\n",
    "        # Node indices (k->j->i) for triplets.\n",
    "        idx_i = col.repeat_interleave(num_triplets)\n",
    "        idx_j = row.repeat_interleave(num_triplets)\n",
    "        idx_k = adj_t_row.storage.col()\n",
    "        mask = idx_i != idx_k  # Remove i == k triplets.\n",
    "        idx_i, idx_j, idx_k = idx_i[mask], idx_j[mask], idx_k[mask]\n",
    "\n",
    "        # Edge indices (k-j, j->i) for triplets.\n",
    "        idx_kj = adj_t_row.storage.value()[mask]\n",
    "        idx_ji = adj_t_row.storage.row()[mask]\n",
    "\n",
    "        return col, row, idx_i, idx_j, idx_k, idx_kj, idx_ji            \n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        z: Tensor,\n",
    "        pos: Tensor,\n",
    "        batch: OptTensor = None,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        # construct edges based on the cutoff decided\n",
    "        # max neighbours should still be under self.max_num_neighbours (Hyperparameter)\n",
    "        edge_index = radius_graph(\n",
    "            pos, r=self.cutoff, batch=batch, max_num_neighbors=self.max_num_neighbors\n",
    "        )\n",
    "        \n",
    "        # get list of triplets\n",
    "        i, j, idx_i, idx_j, idx_k, idx_kj, idx_ji = self.triplets(\n",
    "            edge_index, num_nodes=z.size(0))\n",
    "\n",
    "        # Calculate L2 distances. \n",
    "        dist = (pos[i] - pos[j]).pow(2).sum(dim=-1).sqrt()\n",
    "\n",
    "        # Calculate angles\n",
    "        # first compute the direction j -> i and k -> j\n",
    "        pos_ji, pos_ki = pos[idx_j] - pos[idx_i], pos[idx_k] - pos[idx_j]\n",
    "        # dot product (|x||y|cos\\theta)\n",
    "        a = (pos_ji * pos_ki).sum(dim=-1)\n",
    "        # cross product (|x||y|sin\\theta)\n",
    "        b = torch.cross(pos_ji, pos_ki).norm(dim=-1)\n",
    "        # computes tan inverse of b / a or \n",
    "        angle = torch.atan2(b, a)\n",
    "\n",
    "        rbf = self.rbf(dist)\n",
    "        sbf = self.sbf(dist, angle, idx_kj)\n",
    "\n",
    "        # Embedding block and it's corresponding output\n",
    "        x = self.emb(z, rbf, i, j)\n",
    "        P = self.output_blocks[0](x, rbf, i, num_nodes=pos.size(0))\n",
    "\n",
    "        # Message Passing Interaction blocks\n",
    "        for interaction_block, output_block in zip(self.interaction_blocks,\n",
    "                                                   self.output_blocks[1:]):\n",
    "            x = interaction_block(x, rbf, sbf, idx_kj, idx_ji)\n",
    "            P = P + output_block(x, rbf, i, num_nodes=pos.size(0))\n",
    "\n",
    "        return P.sum(dim=0) if batch is None else scatter(P, batch, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1977e72-0d8a-4e63-953c-a78d9f82bf89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Run on QM9 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1dd27",
   "metadata": {},
   "source": [
    "A quick overview of the training process with the QM9 dataset.\n",
    "\n",
    "1. Initialize the model with hyperparameters based on the paper, and load the dataset.\n",
    "    - Note that we are using `out_channels = 1` since we are training on a single target.\n",
    "2. Set the dataset.data.y to the chosen target. We use `target = 0` for the property $\\mu$.\n",
    "3. Train the model using the usual Pytorch method, and track the mean absolute error for the train and test datasets in each epoch for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fb96179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# initialize dataset\n",
    "dataset = QM9('.')\n",
    "# initialize model\n",
    "model = DimeNet(\n",
    "    hidden_channels=128,\n",
    "    out_channels=1,\n",
    "    num_blocks=6,\n",
    "    num_bilinear=8,\n",
    "    num_spherical=7,\n",
    "    num_radial=6,\n",
    "    cutoff=5.0,\n",
    "    envelope_exponent=5,\n",
    "    num_before_skip=1,\n",
    "    num_after_skip=2,\n",
    "    num_output_layers=3,\n",
    ")\n",
    "\n",
    "# we use the 0th target, for others refer to the original paper\n",
    "target = 0\n",
    "dataset.data.y = dataset.data.y[:, target]\n",
    "\n",
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a426d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same random seed as the official DimeNet` implementation.\n",
    "random_state = np.random.RandomState(seed=42)\n",
    "perm = torch.from_numpy(random_state.permutation(np.arange(130831)))    \n",
    "train_idx = perm[:110000]\n",
    "val_idx = perm[110000:120000]\n",
    "test_idx = perm[120000:]\n",
    "train_dataset, val_dataset, test_dataset = (dataset[train_idx], dataset[val_idx], dataset[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7ac534",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1719/1719 [06:34<00:00,  4.35it/s]\n",
      "100%|██████████| 170/170 [00:13<00:00, 12.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training epoch 0: mean train loss is 5.1584482583249285, mean train mae is 4.963539379876142, mean test mae is 0.15622362069347326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1719/1719 [06:21<00:00,  4.51it/s]\n",
      "100%|██████████| 170/170 [00:12<00:00, 13.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training epoch 1: mean train loss is 0.25099485188531906, mean train mae is 0.14398329698647536, mean test mae is 0.08410120517672862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # store mae in each epoch\n",
    "    epoch_losses = []\n",
    "    epoch_maes = []\n",
    "    for _, data in enumerate(tqdm(train_loader)):\n",
    "        data = data.to(device)\n",
    "        y_true = data.y.unsqueeze(-1)\n",
    "        \n",
    "        # loss \n",
    "        y_pred = model(data.z, data.pos, data.batch)\n",
    "        loss = loss_fn(y_pred, y_true)\n",
    "        \n",
    "        # optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # to prevent exploding gradients, added gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=10, norm_type=2.0\n",
    "        )\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update loss for batch\n",
    "        epoch_losses.append(loss.detach().cpu().item())\n",
    "        \n",
    "        # compute mae\n",
    "        epoch_maes.append(\n",
    "            (y_true.squeeze() - y_pred.squeeze()).mean().abs().item()\n",
    "        )\n",
    "    \n",
    "    # test on test dataloader\n",
    "    test_epoch_maes = []\n",
    "    for _, data in enumerate(tqdm(test_loader)):\n",
    "        data = data.to(device)\n",
    "        y_true = data.y        \n",
    "        # run without grad\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = model(data.z, data.pos, data.batch)\n",
    "        \n",
    "        # compute mae\n",
    "        test_epoch_maes.append(\n",
    "            (y_true.squeeze() - y_test_pred.squeeze()).mean().abs().item()\n",
    "        )\n",
    "    \n",
    "    # compute\n",
    "    print (\n",
    "        f'For training epoch {epoch}: '\n",
    "        f'mean train loss is {np.mean(epoch_losses)}, '\n",
    "        f'mean train mae is {np.mean(epoch_maes)}, '\n",
    "        f'mean test mae is {np.mean(test_epoch_maes)}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc743d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d1354",
   "metadata": {},
   "source": [
    "This blog is inspired from the following sources,\n",
    "* <a href=\"https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/nn/models/dimenet.py\">Pytorch Geometric Model</a>\n",
    "* The original <a href=\"https://arxiv.org/abs/2003.03123\">Directional Message Passing for Molecular Graphs</a> paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48109e18-bef6-4c0c-839c-ab9fb77bab7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c60861-8dcc-431b-b66a-bd127df80bf4",
   "metadata": {},
   "source": [
    "I would like to extend my sincerest gratitude to [Johannes Gasteigger](https://twitter.com/gasteigerjo) for reviewing everything patiently and providing timely feedback. Thank you for your time and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2fd04-3b5e-4f26-a74a-3cf7a2cda725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "94388e816b40c6db6acf0a9c201375cb19712cf7949dbf450144b4e9282e7bbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
