{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01014d5c",
   "metadata": {},
   "source": [
    "## Annotated DimeNet [<a href=\"https://arxiv.org/abs/2003.03123\">Paper</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4565445",
   "metadata": {},
   "source": [
    "This blog will introduce the DimeNet paper (https://arxiv.org/abs/2003.03123, <a href=\"https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/nn/models/dimenet.py\">Pytorch Source Code</a>) through a step-by-step process to build a graph neural network using Pytorch. We'll go through each component of this network and then piece them back together towards the end. Feel free to go through this inside a google colab to play with it.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Graph Neural Networks (GNNs) have now become the go-to architecture for modelling Quantum Mechanical properties of molecules. Earlier GNNs used 2D graph representations of graphs, but with papers like DimeNet that utilize the 3D graph representation of the molecules, the performance on these tasks have been significantly improved. This paper proposes <b> directional message passing </b> that utilizes the positions of atoms in 3D and performs message passing using inter-atomic distances and angles between triplets of atoms.\n",
    "\n",
    "The contributions of this paper are as follows,\n",
    "- A message passing scheme which uses directional information\n",
    "- The directional embeddings are constructed using Spherical Bessel functions and Spherical Harmonics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f9a8a",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "This blog assumes familiarity with message passing in graph neural networks and the usage of PyTorch. For people new to it, feel free to refer to these excellent distil pub articles,\n",
    "- https://distill.pub/2021/gnn-intro/\n",
    "- https://distill.pub/2021/understanding-gnns/\n",
    "- [Scatter Function](https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html#torch_scatter.scatter) for implementing message passing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d8bbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Overview\n",
    "The blog is structured as follows,\n",
    "1. Constructing directional embeddings using 2D spherical Fourier-Bessel Basis Functions\n",
    "2. Architecture of DimeNet and implementation of each component in it\n",
    "3. Running the DimeNet architecture on QM9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe775950",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Package Installation Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04a03e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure there is pyg, torch_geometric, torch, sympy, jupyter\n",
    "# conda create -n annotated-dimenet python=3.8\n",
    "# conda activate annotated-dimenet\n",
    "# conda install pytorch torchvision pytorch-cuda=11.6 -c pytorch -c nvidia\n",
    "# pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
    "# pip install torch_geometric\n",
    "# conda install sympy\n",
    "# pip install jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2df8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06df215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/arc/project/st-jiaruid-1/miniconda3/envs/graph/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from math import pi as PI\n",
    "from math import sqrt\n",
    "from typing import Callable, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, Linear\n",
    "from torch_scatter import scatter\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "from torch_geometric.data import Dataset, download_url\n",
    "from torch_geometric.data.makedirs import makedirs\n",
    "from torch_geometric.nn import radius_graph\n",
    "from torch_geometric.nn.inits import glorot_orthogonal\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.typing import OptTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d94223-d628-409d-a0aa-45f7ad649c81",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Directional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c952e7b-47ae-4da7-a882-59141ae63ab9",
   "metadata": {
    "tags": []
   },
   "source": [
    "The key idea in this paper is to include directional information during message passing. Consider an edge between atom `i` and `j`, the message from `j` to `i` is represented as $m_{ji}$. The message $m_{ji}$ is updated with the help of directional information by considering neighbours of `j` forming an atom triplet `ijk`. For example, we can use the angle between the direction `kj` (from atom `k` to atom `j`) and direction `ji` (from atom `j` to atom `i`) using $\\alpha_{(kj, ji)} = \\angle x_k x_j x_i$.  To update the directional embedding $m_{ji}$, we first consider messages $m_{kj}$ from all the neighbours (see different `k_1, k_2, ..` in the figure below) to `j`. The overall message passing update can then be summarized as follows,\n",
    "\n",
    "<i>Edge Embeddings are updated</i>: Each directional message $m_{ji}$ can be updated from the following three,\n",
    "* incoming messages $m_{kj}$ from neighbours `k`\n",
    "* the directional information $\\alpha_{(kj, ji)}$ \n",
    "* the inter-atomic distance $d_{ij}$. \n",
    "\n",
    "$$\n",
    "h_i = \\sum_{k \\in \\mathcal{N}_i}m_{ki}\n",
    "$$ \n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/message-passing.png\" width=\"200\"/> <br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bd9e3-6d7e-4be2-becd-58b21f50f217",
   "metadata": {},
   "source": [
    "### Representation of inter-atomic distances and bond angles\n",
    "We mentioned how the message passing layer involves inter-atomic distances and bond angles. A natural question then is to ask how the interatomic distance and bond angles are provided to the model. The paper uses the following two methods to transform the raw values,\n",
    "\n",
    "* <i>Radial Basis Function </i>$e_{RBF}^{{ji}}$ for interatomic distances $d_{ji}$\n",
    "* <i>Spherical Basis Function </i>$a_{SBF}^{(kj, ji)}$: A joint angle and distance based function that takes into input the angle $\\alpha_{(kj, ji)}$ and the interatomic distance $d_{kj}$\n",
    "\n",
    "<i>Think of the above two as functions, that transform our input (angles and distances) to a form or space where it is easier for the model to extract relevant information from it.</i>\n",
    "\n",
    "Let's see how these two functions can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17f84b-b825-4366-b55b-bac784354345",
   "metadata": {},
   "source": [
    "### Radial Basis Function\n",
    "The radial basis function used here is an orthogonal basis. It takes into input the interatomic distance $d$, a cutoff distance $c$, \n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{e}}_{RBF}(d) = \\sqrt{\\frac{2}{c}}\\frac{\\sin{(\\frac{n\\pi}{c}}d)}{d}\n",
    "$$\n",
    "\n",
    "* The value of the integer $n$ ranges as $n \\in [1, ... , N_{RBF}]$, where $N_{RBF}$ denotes the number of orthogonal components (bases). \n",
    "* An easier way to think about this is that each inter-atomic distance input $d$ will now be converted into a $N_{RBF}$ sized tensor using the radial basis function.\n",
    "* Using this basis improves parameter efficiency by using 1/4th of the number of parameters used by a Gaussian RBF (Table 3 in the [Paper](https://arxiv.org/abs/2003.03123))\n",
    "<!-- * NOT INCLUDING THIS The paper verifies that this basis function requires 1/4th the number of parameters ($N_{RBF}$) as compared to the gaussian radial basis -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8a2a9-8b31-43fa-93b6-73bb61de66db",
   "metadata": {},
   "source": [
    "### Spherical Basis Function\n",
    "This basis layer is for joint 2D basis for $d_{kj}$ and $\\alpha_{(kj,ji)}$, a function dependent on the interatomic distance and an angle.\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{a}}_{SBF, ln}(d, \\alpha) = \\sqrt{\\frac{2}{c^3 j^2_{l + 1}(z_{ln})}} j_l(\\frac{z_{ln}}{c}d)Y_l^0(\\alpha)\n",
    "$$\n",
    "where $l \\in [0 .. N_{SHBF} - 1]$ and $n \\in [1 ... N_{SRBF}]$. The $z_{ln}$ can be computed using bessel function implementations while $Y_l^0(\\alpha)$ can be computed using spherical harmonics function implementation.\n",
    "\n",
    "<i>An easier way to think about this is that each pair of inter-atomic distance input $d_{ij}$ and angle $\\alpha_{(kj,ji)}$ is converted into a $N_{SRBF} \\times N_{SHBF}$ sized tensor using the radial basis function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652fb56c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Continuous Cutoff using an Envelope Function\n",
    "\n",
    "As we use the 3D structure of the molecule in the model, every inter-atomic pair could be potentially considered for message passing. The downside is that using all inter-atomic pairs could signficantly increase the computational overhead. Instead, we try to use a cutoff ($c$) on the interatomic distance to determine neighbours. \n",
    "\n",
    "A problem that arises with a hard cutoff is that the above functions (radial basis and spherical basis) dependent on $d$ are not twice continuously differentiable anymore. To alleviate this issue, the paper multiplies the $\\tilde{\\mathcal{a}}_{SBF, ln}(d, \\alpha)$ and SBF $\\tilde{\\mathcal{e}}_{RBF}(d)$ with an envelope function $u(d)$ that makes the first and second derivative at the cutoff ($d = c$) as 0.\n",
    "\n",
    "$$\n",
    "\\mathcal{e}_{RBF}(d) = u(d)\\tilde{\\mathcal{e}}_{RBF}(d) \\\\\n",
    "\\mathcal{a}_{SBF}(d) = u(d) \\tilde{\\mathcal{a}}_{SBF}(d)\n",
    "$$\n",
    "\n",
    "This envelope function has the following form with the $p$ in the equation being the exponent,\n",
    "<br/>\n",
    "\n",
    "$$\n",
    "u(d) = 1 - \\frac{(p + 1)(p + 2)}{2}d^p + p(p + 2)d^{p+1} - \\frac{p(p + 1)}{2}d^{p + 2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b556e056-c1ee-44b8-ae75-af188c98670b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Message Passing Equation\n",
    "The message passing equation (Implementation in the Interaction Block Section) would be as follows,\n",
    "\n",
    "$$\n",
    "m_{ji}^{(l + 1)} = f_{\\text{update}}(m_{ji}^{(l)}, \\sum_{k in \\mathcal{N}_j \\backslash \\{i\\}}f_{\\text{int}}(m_{kj}^{(l)}), e_{\\text{RBF}}^{(ji)}, a_{\\text{SBF}}^{(kj, ji)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1fe56d-a50f-495e-968b-68d95027740f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implementation of Spherical Basis Function and Radial Basis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3b970-220c-4cd5-a4ed-ef7e8bca71ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Continuous Cutoff Envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434dae3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "The equation for the continuous envelope\n",
    "\n",
    "$$\n",
    "u(d) = 1 - \\frac{(p + 1)(p + 2)}{2}d^p + p(p + 2)d^{p+1} - \\frac{p(p + 1)}{2}d^{p + 2}\n",
    "$$\n",
    "\n",
    "Some implementation details,\n",
    "- The paper uses a default value of p = 6 or `exponent = 5.` in the code.\n",
    "- We implement this as a `torch.nn.Module()` layer (although no learnable parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f5e8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Envelope(torch.nn.Module):\n",
    "    def __init__(self, exponent: int):\n",
    "        super().__init__()\n",
    "        self.p = exponent + 1\n",
    "        self.a = -(self.p + 1) * (self.p + 2) / 2\n",
    "        self.b = self.p * (self.p + 2)\n",
    "        self.c = -self.p * (self.p + 1) / 2\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        p, a, b, c = self.p, self.a, self.b, self.c\n",
    "        x_pow_p0 = x.pow(p - 1)\n",
    "        x_pow_p1 = x_pow_p0 * x\n",
    "        x_pow_p2 = x_pow_p1 * x\n",
    "        \n",
    "        return (\n",
    "            1. / x + a * x_pow_p0 + b * x_pow_p1 +\n",
    "            c * x_pow_p2) * (x < 1.0).to(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6bed16-0f23-440c-ad98-92f26e228614",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Radial Basis Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db48ec4a",
   "metadata": {},
   "source": [
    "If we recall the equation,\n",
    "$$\n",
    "\\mathcal{e}_{RBF}(d) = u(d)\\tilde{\\mathcal{e}}_{RBF}(d)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{e}}_{RBF}(d) = \\sqrt{\\frac{2}{c}}\\frac{\\sin{(\\frac{n\\pi}{c}}d)}{d} \n",
    "$$\n",
    "\n",
    "$$\n",
    "u(d) : \\text{Envelope Equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "n \\in [1, ... , N_{RBF}]\n",
    "$$\n",
    "\n",
    "where $N_{RBF}$ denotes the number of orthogonal components (bases).\n",
    "\n",
    "The paper implements the Radial Basis Function as a layer where the frequency information is to be learned via backpropagation. We initialize the parameter values initially as $n\\pi/c$.\n",
    "\n",
    "The following mapping from the math variables and to code variables will be used,\n",
    "- $N_{RBF} \\to $ `num_radial` \n",
    "- $c \\to $ `cutoff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdc1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBasisLayer(torch.nn.Module):\n",
    "    '''RadialBasisLayer'''\n",
    "    def __init__(self, num_radial: int, cutoff: float = 5.0,\n",
    "                 envelope_exponent: int = 5):\n",
    "        super().__init__()\n",
    "        # the c in the radial basis layer equation\n",
    "        self.cutoff = cutoff\n",
    "        # u(d) / envelope\n",
    "        self.envelope = Envelope(envelope_exponent)\n",
    "        \n",
    "        # the different frequencies to be considered to generate orthogonal basis\n",
    "        self.freq = torch.nn.Parameter(torch.Tensor(num_radial))\n",
    "        \n",
    "        # make sure we reset_parameters during __init__()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        with torch.no_grad():\n",
    "            torch.arange(1, self.freq.numel() + 1, out=self.freq).mul_(PI)\n",
    "        self.freq.requires_grad_()\n",
    "\n",
    "    def forward(self, dist: Tensor) -> Tensor:\n",
    "        # compute d = (d/c)\n",
    "        dist = (dist.unsqueeze(-1) / self.cutoff)\n",
    "        # compute u(d/c) * sin(\\frac{n\\pi}{c} \\times d)\n",
    "        return self.envelope(dist) * (self.freq * dist).sin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518bd40d-c32d-4dfe-a840-669ce54723d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Spherical Basis Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c5b16",
   "metadata": {},
   "source": [
    "If we recall the equation,\n",
    "\n",
    "$$\n",
    "\\mathcal{a}_{RBF}(d) = u(d) \\tilde{\\mathcal{a}}_{RBF}(d) \\\\\n",
    "$$\n",
    "$$\n",
    "\\tilde{\\mathcal{a}}_{SBF, ln}(d, \\alpha) = \\sqrt{\\frac{2}{c^3 j^2_{l + 1}(z_{ln})}} j_l(\\frac{z_{ln}}{c}d)Y_l^0(\\alpha)\n",
    "$$\n",
    "where $l \\in [0 .. N_{SHBF} - 1]$ and $n \\in [1 ... N_{SRBF}]$\n",
    "\n",
    "The following mapping from the math variables and to code variables will be used,\n",
    "* $N_{SHBF} \\to $ `num_spherical` \n",
    "* $N_{SRBF} \\to $ `num_radial` \n",
    "* $c \\to $ `cutoff`\n",
    "\n",
    "The following two variables represent,\n",
    "* $z_{ln}$: $n'$th root of the $l$-order Bessel Functions\n",
    "* $Y_l^0(\\alpha)$: Special Harmonics\n",
    "\n",
    "The two variables $z_{ln}$ and $Y_l^0(\\alpha)$ in the spherical basis layer equation have been implemented in `torch_geometric.nn.models.dimenet_utils` as `bessel_basis` and `real_sph_harm`. Exact details of implementing these functions using `sympy`'s <a href=\"https://docs.sympy.org/latest/index.html\">symbolic computation library</a> is outside the scope of this blog. However, some points over their computation, \n",
    "- $z_{ln}$ has a total of $N_{SHBF} \\times N_{SRBF}$ values as it varies with both $l$ and $n$ whereas $Y_l^0(\\alpha)$ has $N_{SHBF}$ values as it varies with $l$\n",
    "- The output of these functions are sympy expressions i.e. it will have symbols like `sin()`, `cos()` and `x`\n",
    "- To convert any symbolic expresssion involving (`x`, `sin` and `cos`) into a lambda function, we use `sym.lambdify([x], expression, modules)` [<a href=\"https://docs.sympy.org/latest/modules/utilities/lambdify.html#sympy.utilities.lambdify.lambdify\">Link</a>], the modules here maps the symbolic function `sin` and `cos` to our torch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8d1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "\n",
    "from torch_geometric.nn.models.dimenet_utils import (\n",
    "    bessel_basis, # SRBF\n",
    "    real_sph_harm, # SHBF\n",
    ")\n",
    "\n",
    "class SphericalBasisLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_spherical: int, \n",
    "        num_radial: int,\n",
    "        cutoff: float = 5.0, \n",
    "        envelope_exponent: int = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_radial <= 64\n",
    "        self.num_spherical = num_spherical\n",
    "        self.num_radial = num_radial\n",
    "        self.cutoff = cutoff\n",
    "        self.envelope = Envelope(envelope_exponent)\n",
    "        \n",
    "        # We can bessel basis and spherical Harmonic forms in sympy expressions\n",
    "        # sympy expressions -> Equations containing `sin` `cos` `x` and `theta`\n",
    "        # computing z_{ln} constant, based on l(num_spherical) and n(num_radial)\n",
    "        bessel_forms = bessel_basis(num_spherical, num_radial)\n",
    "        # computing Y_l^0(\\alpha)\n",
    "        sph_harm_forms = real_sph_harm(num_spherical)\n",
    "        \n",
    "        # let's fill spherical and radial functions for \n",
    "        # l \\in [0, ..., N_SHBF - 1]\n",
    "        # n \\in [1, ..., N_SRBF]\n",
    "        \n",
    "        # spherical functions only dependent on l, there will be N_SHBF of them\n",
    "        self.sph_funcs = []\n",
    "        # bessel functions dependent on l and n, there will be N_SHBF x N_SRBF of them\n",
    "        self.bessel_funcs = []\n",
    "            \n",
    "        # Using Sympy, we convert sympy expressions into lambda functions\n",
    "        x, theta = sym.symbols('x theta')\n",
    "        modules = {'sin': torch.sin, 'cos': torch.cos}\n",
    "        \n",
    "        # i goes from 0 to num_spherical x-x 1 (exactly the range of l)\n",
    "        for i in range(num_spherical):\n",
    "            if i == 0:\n",
    "                sph1 = sym.lambdify([theta], sph_harm_forms[i][0], modules)(0)\n",
    "                self.sph_funcs.append(lambda x: torch.zeros_like(x) + sph1)\n",
    "            else:\n",
    "                sph = sym.lambdify([theta], sph_harm_forms[i][0], modules)\n",
    "                self.sph_funcs.append(sph)\n",
    "                \n",
    "            for j in range(num_radial):\n",
    "                bessel = sym.lambdify([x], bessel_forms[i][j], modules)\n",
    "                self.bessel_funcs.append(bessel)\n",
    "\n",
    "    def forward(self, dist: Tensor, angle: Tensor, idx_kj: Tensor) -> Tensor:\n",
    "        '''Performs Forward Pass'''\n",
    "        # computes d / c\n",
    "        dist = dist / self.cutoff\n",
    "        \n",
    "        # n, k = self.num_spherical, self.num_radial\n",
    "        # computes over all radial bessel (n x k) functions and stack over feature dimension (1)\n",
    "        rbf = torch.stack([f(dist) for f in self.bessel_funcs], dim=1)\n",
    "        # multiply with u(d) * bessel() \n",
    "        # Since u(d) will be of same shape as d, u(d).unsqueeze(-1) to allow dot product\n",
    "        rbf = self.envelope(dist).unsqueeze(-1) * rbf\n",
    "        \n",
    "        # compute over all spherical (n) functions and stack over feature dimension (1)\n",
    "        sbf = torch.stack([f(angle) for f in self.sph_funcs], dim=1)\n",
    "        \n",
    "        n, k = self.num_spherical, self.num_radial\n",
    "        # multiply the two to get a_SBF = u(d) * radial_bessel() * spherical_harmonics()\n",
    "        out = (rbf[idx_kj].view(-1, n, k) * sbf.view(-1, n, 1)).view(-1, n * k)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87712b26-691c-4962-bf83-8d99ec345af2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Architecture of DimeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67d628-bb0f-477e-bd68-6161679b90ce",
   "metadata": {},
   "source": [
    "The architecture diagram of DimeNet from the paper. The architecture consists of the following,\n",
    "- RBF and SBF to transform our interatomic distances and bond angles (Implementation covered above)\n",
    "- Embedding Blocks to Initialize initial messages embeddings\n",
    "- (Multiple) Interaction Blocks to perform Message Passing\n",
    "- Output Block to convert embeddings to generate a prediction\n",
    "\n",
    "<center>\n",
    "<img src='imgs/dimenet-arch.png' width='600'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde5b0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<b> High Level Overview of Network Architecture</b><br>\n",
    "In the process let's try to informally write down the `forward()` function. The high-level structure of the architecture is as follows,\n",
    "<center>\n",
    "<img src=\"imgs/model-only.png\" width=\"200\"/>\n",
    "</center>\n",
    "\n",
    "* Given the number of nodes `num_nodes` and edge_index `edge_index`, get all triplets of form `ijk` and edge indices of `ji` and `kj`\n",
    "* From each triplet, we can get the distance of the `ij` pair and the angle `ijk` with the help of the coordinates (refer to the figure below),\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/angle-distance.png\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "* Convert these distances and angles into a tensor\n",
    "* Transform inter-atomic distance using the radial basis function `self.rbf(d)`\n",
    "* Transform inter-atomic distance and the angle using spherical basis function `self.sbf(d, angle)`\n",
    "* The outputs are constructed from `1 Embedding Block` and `6 Interaction Blocks` (Message Passing Layer) in a sequential manner where\n",
    "    * first output prediction from `embedding + output` block using initial input embeddings\n",
    "    * all other output predictions from `interaction + output` block using embeddings from previous layer  \n",
    "* All the outputs are summed up to provide the final scalar prediction per molecule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844d981-e44e-4032-8e99-a17ec133af9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Embedding Block: Getting Message Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4853b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's have a closer look at the embedding block. \n",
    "<center>\n",
    "<img src='imgs/embedding-block.png' width='400'/>\n",
    "</center>\n",
    "\n",
    "The following steps happen,\n",
    "1. Four inputs to this embedding block (look at `forward()` function below)\n",
    "    - Atomic Numbers of the atoms in the batch (x)\n",
    "    - Distance between i and j atom transformed via RBF (input feature dimension `num_radial`)\n",
    "    - atomic number of i\n",
    "    - atomic number of j\n",
    "2. Apply `Linear(num_radial, hidden_channels)` on the RBF output, get distance embeddings\n",
    "3. Transform atomic numbers into learnable embeddings using `nn.Embedding(95, hidden_channels)` (where 95 is the maximum atomic number that can be expected)\n",
    "3. Concatenate atom embeddings from `x_i`, `x_j` and `e_{RBF}` distance embedding along `dim=1` (shape: `3 x hidden_channels`)\n",
    "4. Apply `Linear(3 * hidden_channels, hidden_channels)` to concatenate representation. This is the edge embedding between atom `i` and `j`.\n",
    "5. The final message along with the rbf input is provided to an `Output Block` (covered later) to get a scalar prediction for each atom $t_i^{(1)}$ (This step won't be a part of the code implementation of embedding block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a30d1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(torch.nn.Module):\n",
    "    '''Implementation of Embedding Block\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_radial: int\n",
    "        Number of radial features (feature dimensions of rbf output)\n",
    "    hidden_channels: int\n",
    "        feature dimension of output of this embedding block\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, num_radial: int, hidden_channels: int, act: Callable):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        \n",
    "        self.emb = Embedding(95, hidden_channels)\n",
    "        self.lin_rbf = Linear(num_radial, hidden_channels)\n",
    "        self.lin = Linear(3 * hidden_channels, hidden_channels)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.emb.weight.data.uniform_(-sqrt(3), sqrt(3))\n",
    "        self.lin_rbf.reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor, j: Tensor) -> Tensor:\n",
    "        x = self.emb(x)\n",
    "        rbf = self.act(self.lin_rbf(rbf))\n",
    "        return self.act(self.lin(torch.cat([x[i], x[j], rbf], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d960a54-e664-4e50-b0fa-fdd6508df2ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Residual Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a13e3-8529-45df-9095-ac12950d18fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "It is a part of the Interaction Block. Inspired by residual blocks in ResNets which improve the flow of gradients, the interaction block (discussed next) makes use of residual blocks in its architecture. The architecture of residual blocks is as follows,\n",
    "\n",
    "<center>\n",
    "<img src='imgs/residual.png' width='200'/>\n",
    "</center>\n",
    "\n",
    "Implementation is fairly simple,\n",
    "- 2 `nn.Linear` layers that do not change the input feature dimension\n",
    "- We apply the 2 linear layers with non-linear activations in between and add the initial input (`x`) and the output (`act(lin_2(act(lin_1(x))))`) as the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73e8580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels: int, act: Callable):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot_orthogonal(self.lin1.weight, scale=2.0)\n",
    "        self.lin1.bias.data.fill_(0)\n",
    "        glorot_orthogonal(self.lin2.weight, scale=2.0)\n",
    "        self.lin2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x + self.act(self.lin2(self.act(self.lin1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079aa4a0-136d-42c1-8fd8-49d0327d7b9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Interaction Block: Message Passing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bc26b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "This block implements the main message passing equation,\n",
    "\n",
    "$$\n",
    "m_{ji}^{(l + 1)} = f_{\\text{update}}(m_{ji}^{(l)}, \\sum_{k in \\mathcal{N}_j \\backslash \\{i\\}}f_{\\text{int}}(m_{kj}^{(l)}), e_{\\text{RBF}}^{(ji)}, a_{\\text{SBF}}^{(kj, ji)})\n",
    "$$\n",
    "\n",
    "<center>\n",
    "    <img src='imgs/interaction-block.png' width='500'/>\n",
    "</center>\n",
    "\n",
    "\n",
    "Some points over the implementation of the interaction block,\n",
    "1. The inputs to this block are,\n",
    "    - input embedding features `x` (from the embedding/interaction block before)\n",
    "    - Radial Basis Function input (input feature dimension: `num_radial`)\n",
    "    - Spherical Basis Function embeddings (input feature dimension: `num_spherical * num_radial`)\n",
    "    - `idx_kj`: All indices from atom `k` to atom `j`\n",
    "    - `idx_ji`: All indices from atom `j` to atom `i`\n",
    "2. Apply `Linear(num_radial, hidden_channels, bias=False)` to $e_{\\text{RBF}}$\n",
    "3. $a_{\\text{SBF}}$ is transformed into a $N_\\text{bilinear}$-representation using `Linear(num_radial * num_spherical, num_bilinear, bias=False)`\n",
    "3. To compute the message to be calculated,\n",
    "    - transform the messages $x_{ij}$ and $x_{kj}$ using `activation(linear(hidden_channels, hidden_channels))`\n",
    "    - Dot product between `linear(rbf)` and above transformed $x_{kj}$\n",
    "    - Bilinear product $a_{sbf}^T\\cdot W \\cdot x_{kj}$\n",
    "    - message passing using `torch_scatter.scatter` to get $\\sum_k {m_{kj}}$\n",
    "4. Post Message Passing, update our $m_{ji}^{(l)} = \\text{Swish Activation}(\\text{linear}(m_{ji}^{(l - 1)})) + \\sum_k {m_{kj}}$ \n",
    "5. Run this through some `Residual Layer` (architecture explained next)\n",
    "6. Pass the final updated message along with RBF input into the `Output Layer` (explained later) to get the target scalar value for that interaction block $l$ and each atom $i$ as $t_i^{(l)}$ (This step won't be a part of the code implementation of interaction block)\n",
    "\n",
    "<b>Note</b>: [Swish](https://arxiv.org/abs/1710.05941v2) / [SiLU](https://arxiv.org/abs/1702.03118) works much better than activation functions used in previous works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e401ac9-41d6-49d5-a2fb-e79e4d2eccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionBlock(torch.nn.Module):\n",
    "    '''Interaction Block in DimeNet: Reponsible for Message Passing'''\n",
    "    def __init__(self, hidden_channels: int, num_bilinear: int,\n",
    "                 num_spherical: int, num_radial: int, num_before_skip: int,\n",
    "                 num_after_skip: int, act: Callable):\n",
    "        '''\n",
    "        Initialize Interaction Module\n",
    "        \n",
    "        Args:\n",
    "            hidden_channels (int): Hidden embedding size.\n",
    "            num_bilinear (int): Size of the bilinear layer tensor.\n",
    "            num_spherical (int): Number of spherical harmonics.\n",
    "            num_radial (int): Number of radial basis functions.\n",
    "            num_before_skip (int, optional): Number of residual layers in the\n",
    "                interaction blocks before the skip connection. (default: :obj:`1`)\n",
    "            num_after_skip (int, optional): Number of residual layers in the\n",
    "                interaction blocks after the skip connection. (default: :obj:`2`)\n",
    "            act (str or Callable, optional): The activation function.\n",
    "                (default: :obj:`\"swish\"`)        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "\n",
    "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
    "        self.lin_sbf = Linear(num_spherical * num_radial, num_bilinear,\n",
    "                              bias=False)\n",
    "\n",
    "        # Dense transformations of input messages.\n",
    "        self.lin_kj = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin_ji = Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.W = torch.nn.Parameter(\n",
    "            torch.Tensor(hidden_channels, num_bilinear, hidden_channels))\n",
    "\n",
    "        self.layers_before_skip = torch.nn.ModuleList([\n",
    "            ResidualLayer(hidden_channels, act) for _ in range(num_before_skip)\n",
    "        ])\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.layers_after_skip = torch.nn.ModuleList([\n",
    "            ResidualLayer(hidden_channels, act) for _ in range(num_after_skip)\n",
    "        ])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        '''Initializing Parameters'''\n",
    "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_sbf.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_kj.weight, scale=2.0)\n",
    "        self.lin_kj.bias.data.fill_(0)\n",
    "        glorot_orthogonal(self.lin_ji.weight, scale=2.0)\n",
    "        self.lin_ji.bias.data.fill_(0)\n",
    "        self.W.data.normal_(mean=0, std=2 / self.W.size(0))\n",
    "        for res_layer in self.layers_before_skip:\n",
    "            res_layer.reset_parameters()\n",
    "        glorot_orthogonal(self.lin.weight, scale=2.0)\n",
    "        self.lin.bias.data.fill_(0)\n",
    "        for res_layer in self.layers_after_skip:\n",
    "            res_layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Tensor, rbf: Tensor, sbf: Tensor, idx_kj: Tensor,\n",
    "                idx_ji: Tensor) -> Tensor:\n",
    "        # transform rbf and sbf input using their respective nn.Linear()\n",
    "        rbf = self.lin_rbf(rbf)\n",
    "        sbf = self.lin_sbf(sbf)\n",
    "        \n",
    "        # Transform the messages into activation(linear(message))\n",
    "        x_ji = self.act(self.lin_ji(x))\n",
    "        x_kj = self.act(self.lin_kj(x))\n",
    "        x_kj = x_kj * rbf\n",
    "        \n",
    "        # bilinear product\n",
    "        x_kj = torch.einsum('wj,wl,ijl->wi', sbf, x_kj[idx_kj], self.W)\n",
    "        \n",
    "        # message passing\n",
    "        x_kj = scatter(x_kj, idx_ji, dim=0, dim_size=x.size(0))\n",
    "        \n",
    "        # update our message\n",
    "        h = x_ji + x_kj\n",
    "        \n",
    "        # Apply residual layers\n",
    "        for layer in self.layers_before_skip:\n",
    "            h = layer(h)\n",
    "        h = self.act(self.lin(h)) + x\n",
    "        for layer in self.layers_after_skip:\n",
    "            h = layer(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a16ba-0f22-4c23-9104-cd3b34bf914b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Output Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63770bd2",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='imgs/output-block.png' width='100'/>\n",
    "</center>\n",
    "\n",
    "The output block is applied after the embedding block and each interaction block. This is responsible for calculating the final scalar target $t_i^{(l)}$ for each atom ($i$) for that interaction block ($l$) or Embedding Block. For each atom $i$, if we consider all the messages from neighbours $j \\in \\mathcal{N}_i$ and radial basis function outputs $e_{RBF}^{ji}$, the scalar output $t_i^{(l)}$ for that atom $i$ and interaction block $l$ can be calculated as follows,\n",
    "- Transform the RBF input using a Linear weight\n",
    "- Dot product between the transformed RBF input and input messages (efficient way to do this for each atom $i$ is using the `torch_scatter.scatter()`)\n",
    "- Forward pass through standard feedforward neural network layers\n",
    "\n",
    "<b>Note</b>: The dot product between the transformed RBF output and input messages guarantee a twice differentiable output because of the envelope function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab933f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputBlock(torch.nn.Module):\n",
    "    def __init__(self, num_radial: int, hidden_channels: int,\n",
    "                 out_channels: int, num_layers: int, act: Callable):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        \n",
    "        # Linear layer to convert rbf input\n",
    "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
    "        \n",
    "        # linear layers to convert output through num_layers\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.lins.append(Linear(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # final linear layer to convert it into `out_channels` dim output\n",
    "        self.lin = Linear(hidden_channels, out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
    "        for lin in self.lins:\n",
    "            glorot_orthogonal(lin.weight, scale=2.0)\n",
    "            lin.bias.data.fill_(0)\n",
    "        self.lin.weight.data.fill_(0)\n",
    "\n",
    "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor,\n",
    "                num_nodes: Optional[int] = None) -> Tensor:\n",
    "        x = self.lin_rbf(rbf) * x\n",
    "\n",
    "        # scatter-add to add messages from all m_{ji} to atom $i$\n",
    "        x = scatter(x, i, dim=0, dim_size=num_nodes)\n",
    "        \n",
    "        # pass the x through multiple linear layers\n",
    "        for lin in self.lins:\n",
    "            x = self.act(lin(x))\n",
    "        \n",
    "        # pass it through the final output layer\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11405f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Final Prediction\n",
    "The final prediction is calculated as follows, summation of outputs from the output blocks of all the interaction layers and the embedding layer.\n",
    "\n",
    "$$\n",
    "t = \\sum_i \\sum_l t_{i}^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd5fbd-d817-41e3-8349-b338a7302e27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DimeNet: Putting it All Together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee350b",
   "metadata": {
    "tags": []
   },
   "source": [
    "A quick recap of the forward pass steps here,\n",
    "* Given the molecule structure, transform the distance and angle for each atom triplet using the radial basis function `self.rbf(d)` and spherical basis function `self.sbf(d, angle)`.\n",
    "* Construct the output from the output blocks of `1 Embedding Block` and `6 Interaction Blocks` using initial embeddings. This is done in a sequential manner.\n",
    "* Sum the outputs to get a final scalar prediction per molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be2de2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimeNet(torch.nn.Module):\n",
    "    r\"\"\"The directional message passing neural network (DimeNet) from the\n",
    "    `\"Directional Message Passing for Molecular Graphs\"\n",
    "    <https://arxiv.org/abs/2003.03123>`_ paper.\n",
    "    DimeNet transforms messages based on the angle between them in a\n",
    "    rotation-equivariant fashion.\n",
    "\n",
    "    Args:\n",
    "        hidden_channels (int): Hidden embedding size.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_blocks (int): Number of building blocks.\n",
    "        num_bilinear (int): Size of the bilinear layer tensor.\n",
    "        num_spherical (int): Number of spherical harmonics.\n",
    "        num_radial (int): Number of radial basis functions.\n",
    "        cutoff (float, optional): Cutoff distance for interatomic\n",
    "            interactions. (default: :obj:`5.0`)\n",
    "        max_num_neighbors (int, optional): The maximum number of neighbors to\n",
    "            collect for each node within the :attr:`cutoff` distance.\n",
    "            (default: :obj:`32`)\n",
    "        envelope_exponent (int, optional): Shape of the smooth cutoff.\n",
    "            (default: :obj:`5`)\n",
    "        num_before_skip (int, optional): Number of residual layers in the\n",
    "            interaction blocks before the skip connection. (default: :obj:`1`)\n",
    "        num_after_skip (int, optional): Number of residual layers in the\n",
    "            interaction blocks after the skip connection. (default: :obj:`2`)\n",
    "        num_output_layers (int, optional): Number of linear layers for the\n",
    "            output blocks. (default: :obj:`3`)\n",
    "        act (str or Callable, optional): The activation function.\n",
    "            (default: :obj:`\"swish\"`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        num_blocks: int,\n",
    "        num_bilinear: int,\n",
    "        num_spherical: int,\n",
    "        num_radial,\n",
    "        cutoff: float = 5.0,\n",
    "        max_num_neighbors: int = 32,\n",
    "        envelope_exponent: int = 5,\n",
    "        num_before_skip: int = 1,\n",
    "        num_after_skip: int = 2,\n",
    "        num_output_layers: int = 3,\n",
    "        act: Union[str, Callable] = 'swish',\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_spherical < 2:\n",
    "            raise ValueError(\"num_spherical should be greater than 1\")\n",
    "        \n",
    "        # get activation function for the `act` string passing in `__init__`\n",
    "        act = activation_resolver(act)\n",
    "\n",
    "        # cutoff value $c$ below which we consider atoms to be neighbours\n",
    "        self.cutoff = cutoff\n",
    "        # If neighbours exceed this we consider top \n",
    "        # max_num_neighbours based on inter-atomic distance\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        \n",
    "        # number of interaction blocks/message passing blocks\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        # Our radial basis layer for inter-atomic distance\n",
    "        self.rbf = RadialBasisLayer(num_radial, cutoff, envelope_exponent)\n",
    "\n",
    "        # Spherical Basis Layer for 2D joint representation\n",
    "        # using distance and angle\n",
    "        self.sbf = SphericalBasisLayer(num_spherical, num_radial, cutoff,\n",
    "                                       envelope_exponent)\n",
    "        \n",
    "        # embedding block\n",
    "        self.emb = EmbeddingBlock(num_radial, hidden_channels, act)\n",
    "        \n",
    "        # embedding blocks\n",
    "        self.output_blocks = torch.nn.ModuleList([\n",
    "            OutputBlock(num_radial, hidden_channels, out_channels,\n",
    "                        num_output_layers, act) for _ in range(num_blocks + 1)\n",
    "        ])\n",
    "\n",
    "        self.interaction_blocks = torch.nn.ModuleList([\n",
    "            InteractionBlock(hidden_channels, num_bilinear, num_spherical,\n",
    "                             num_radial, num_before_skip, num_after_skip, act)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.rbf.reset_parameters()\n",
    "        self.emb.reset_parameters()\n",
    "        for out in self.output_blocks:\n",
    "            out.reset_parameters()\n",
    "        for interaction in self.interaction_blocks:\n",
    "            interaction.reset_parameters()\n",
    "\n",
    "    def triplets(\n",
    "        self,\n",
    "        edge_index: Tensor,\n",
    "        num_nodes: int,\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
    "        '''Get kji triplets for directional message passing'''\n",
    "        row, col = edge_index  # j->i\n",
    "        \n",
    "        value = torch.arange(row.size(0), device=row.device)\n",
    "        \n",
    "        # get sparse adjacency matrix\n",
    "        adj_t = SparseTensor(row=col, col=row, value=value,\n",
    "                             sparse_sizes=(num_nodes, num_nodes))\n",
    "        adj_t_row = adj_t[row]\n",
    "        num_triplets = adj_t_row.set_value(None).sum(dim=1).to(torch.long)\n",
    "\n",
    "        # Node indices (k->j->i) for triplets.\n",
    "        idx_i = col.repeat_interleave(num_triplets)\n",
    "        idx_j = row.repeat_interleave(num_triplets)\n",
    "        idx_k = adj_t_row.storage.col()\n",
    "        mask = idx_i != idx_k  # Remove i == k triplets.\n",
    "        idx_i, idx_j, idx_k = idx_i[mask], idx_j[mask], idx_k[mask]\n",
    "\n",
    "        # Edge indices (k-j, j->i) for triplets.\n",
    "        idx_kj = adj_t_row.storage.value()[mask]\n",
    "        idx_ji = adj_t_row.storage.row()[mask]\n",
    "\n",
    "        return col, row, idx_i, idx_j, idx_k, idx_kj, idx_ji            \n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        z: Tensor,\n",
    "        pos: Tensor,\n",
    "        batch: OptTensor = None,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        # construct edges based on the cutoff decided\n",
    "        # max neighbours should still be under self.max_num_neighbours (Hyperparameter)\n",
    "        edge_index = radius_graph(\n",
    "            pos, r=self.cutoff, batch=batch, max_num_neighbors=self.max_num_neighbors\n",
    "        )\n",
    "        \n",
    "        # get list of triplets\n",
    "        i, j, idx_i, idx_j, idx_k, idx_kj, idx_ji = self.triplets(\n",
    "            edge_index, num_nodes=z.size(0))\n",
    "\n",
    "        # Calculate L2 distances. \n",
    "        dist = (pos[i] - pos[j]).pow(2).sum(dim=-1).sqrt()\n",
    "\n",
    "        # Calculate angles\n",
    "        # first compute the direction j -> i and k -> j\n",
    "        pos_ji, pos_ki = pos[idx_j] - pos[idx_i], pos[idx_k] - pos[idx_j]\n",
    "        # dot product (|x||y|cos\\theta)\n",
    "        a = (pos_ji * pos_ki).sum(dim=-1)\n",
    "        # cross product (|x||y|sin\\theta)\n",
    "        b = torch.cross(pos_ji, pos_ki).norm(dim=-1)\n",
    "        # computes tan inverse of b / a or \n",
    "        angle = torch.atan2(b, a)\n",
    "\n",
    "        rbf = self.rbf(dist)\n",
    "        sbf = self.sbf(dist, angle, idx_kj)\n",
    "\n",
    "        # Embedding block and it's corresponding output\n",
    "        x = self.emb(z, rbf, i, j)\n",
    "        P = self.output_blocks[0](x, rbf, i, num_nodes=pos.size(0))\n",
    "\n",
    "        # Message Passing Interaction blocks\n",
    "        for interaction_block, output_block in zip(self.interaction_blocks,\n",
    "                                                   self.output_blocks[1:]):\n",
    "            x = interaction_block(x, rbf, sbf, idx_kj, idx_ji)\n",
    "            P = P + output_block(x, rbf, i, num_nodes=pos.size(0))\n",
    "\n",
    "        return P.sum(dim=0) if batch is None else scatter(P, batch, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1977e72-0d8a-4e63-953c-a78d9f82bf89",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run on QM9 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1dd27",
   "metadata": {},
   "source": [
    "The following steps are to train on QM9 dataset. We display training only on a single target (target = 0 or $\\mu$ in the QM9 dataset). We perform the following steps,\n",
    "\n",
    "1. initialize model (`hyperparams` based on paper) and dataset\n",
    "    - `out_channels = 1`, since we are doing single target\n",
    "2. We would be doing single target training ($\\text{target} = 0$), set the `dataset.data.y` based on this `target`\n",
    "3. Train the model Usual Pytorch Way and Track Mean Absolute Error for train and test dataset in each epoch for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fb96179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# initialize dataset\n",
    "dataset = QM9('.')\n",
    "# initialize model\n",
    "model = DimeNet(\n",
    "    hidden_channels=128,\n",
    "    out_channels=1,\n",
    "    num_blocks=6,\n",
    "    num_bilinear=8,\n",
    "    num_spherical=7,\n",
    "    num_radial=6,\n",
    "    cutoff=5.0,\n",
    "    envelope_exponent=5,\n",
    "    num_before_skip=1,\n",
    "    num_after_skip=2,\n",
    "    num_output_layers=3,\n",
    ")\n",
    "\n",
    "# we use the 0th target, for others refer to the original paper\n",
    "target = 0\n",
    "dataset.data.y = dataset.data.y[:, target]\n",
    "\n",
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a426d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same random seed as the official DimeNet` implementation.\n",
    "random_state = np.random.RandomState(seed=42)\n",
    "perm = torch.from_numpy(random_state.permutation(np.arange(130831)))    \n",
    "train_idx = perm[:110000]\n",
    "val_idx = perm[110000:120000]\n",
    "test_idx = perm[120000:]\n",
    "train_dataset, val_dataset, test_dataset = (dataset[train_idx], dataset[val_idx], dataset[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7ac534",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1719/1719 [06:34<00:00,  4.35it/s]\n",
      "100%|| 170/170 [00:13<00:00, 12.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training epoch 0: mean train loss is 5.1584482583249285, mean train mae is 4.963539379876142, mean test mae is 0.15622362069347326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1719/1719 [06:21<00:00,  4.51it/s]\n",
      "100%|| 170/170 [00:12<00:00, 13.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training epoch 1: mean train loss is 0.25099485188531906, mean train mae is 0.14398329698647536, mean test mae is 0.08410120517672862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # store mae in each epoch\n",
    "    epoch_losses = []\n",
    "    epoch_maes = []\n",
    "    for _, data in enumerate(tqdm(train_loader)):\n",
    "        data = data.to(device)\n",
    "        y_true = data.y.unsqueeze(-1)\n",
    "        \n",
    "        # loss \n",
    "        y_pred = model(data.z, data.pos, data.batch)\n",
    "        loss = loss_fn(y_pred, y_true)\n",
    "        \n",
    "        # optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # to prevent exploding gradients, added gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=10, norm_type=2.0\n",
    "        )\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update loss for batch\n",
    "        epoch_losses.append(loss.detach().cpu().item())\n",
    "        \n",
    "        # compute mae\n",
    "        epoch_maes.append(\n",
    "            (y_true.squeeze() - y_pred.squeeze()).mean().abs().item()\n",
    "        )\n",
    "    \n",
    "    # test on test dataloader\n",
    "    test_epoch_maes = []\n",
    "    for _, data in enumerate(tqdm(test_loader)):\n",
    "        data = data.to(device)\n",
    "        y_true = data.y        \n",
    "        # run without grad\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = model(data.z, data.pos, data.batch)\n",
    "        \n",
    "        # compute mae\n",
    "        test_epoch_maes.append(\n",
    "            (y_true.squeeze() - y_test_pred.squeeze()).mean().abs().item()\n",
    "        )\n",
    "    \n",
    "    # compute\n",
    "    print (\n",
    "        f'For training epoch {epoch}: '\n",
    "        f'mean train loss is {np.mean(epoch_losses)}, '\n",
    "        f'mean train mae is {np.mean(epoch_maes)}, '\n",
    "        f'mean test mae is {np.mean(test_epoch_maes)}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc743d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d1354",
   "metadata": {},
   "source": [
    "This blog is inspired from the following sources,\n",
    "* <a href=\"https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/nn/models/dimenet.py\">Pytorch Geometric Model</a>\n",
    "* The original <a href=\"https://arxiv.org/abs/2003.03123\">Directional Message Passing for Molecular Graphs</a> paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48109e18-bef6-4c0c-839c-ab9fb77bab7e",
   "metadata": {},
   "source": [
    "### Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c60861-8dcc-431b-b66a-bd127df80bf4",
   "metadata": {},
   "source": [
    "A big thanks to [Johannes Gasteigger](https://twitter.com/gasteigerjo) for reviewing and giving feedback on this blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2fd04-3b5e-4f26-a74a-3cf7a2cda725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "94388e816b40c6db6acf0a9c201375cb19712cf7949dbf450144b4e9282e7bbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
